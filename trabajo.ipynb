{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boletin 1 Practice Report\n",
    "This notebook is my personal record of the Boletin 1 work. I explain every task in simple English so I can present it later in class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.cluster import AgglomerativeClustering, DBSCAN, KMeans\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    completeness_score,\n",
    "    homogeneity_score,\n",
    "    mutual_info_score,\n",
    "    rand_score,\n",
    "    silhouette_score,\n",
    "    v_measure_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from class_helpers import (\n",
    "    ensure_practice_paths,\n",
    "    grid_search_kmeans,\n",
    "    load_zoo,\n",
    "    paths,\n",
    "    scale_features,\n",
    ")\n",
    "\n",
    "ensure_practice_paths()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Zoo K-Means study\n",
    "I start with the zoo table from the practice files. I remove the type column, scale the numeric fields, and test several cluster counts and seeds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo_data = load_zoo(include_type=False)\n",
    "X_zoo = zoo_data.features\n",
    "y_zoo = zoo_data.labels\n",
    "scaler_zoo, X_zoo_scaled = scale_features(X_zoo)\n",
    "zoo_data.table.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preview confirms that the animal names and the feature flags are loaded correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [5, 6, 7, 8]\n",
    "seed_values = [0, 1, 2]\n",
    "results_kmeans = grid_search_kmeans(X_zoo_scaled, y_zoo, k_values, seed_values)\n",
    "results_kmeans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table shows inertia, silhouette, and adjusted rand index for each combination. I see that k equal to seven with seed zero gives the best adjusted rand index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_row = results_kmeans.sort_values(\"ARI\", ascending=False).iloc[0]\n",
    "best_k = int(best_row[\"k\"])\n",
    "best_seed = int(best_row[\"seed\"])\n",
    "best_k, best_seed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I fit one final K-Means model with that choice and compare the cluster labels with the true animal types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_kmeans = KMeans(n_clusters=best_k, random_state=best_seed, n_init=10)\n",
    "final_labels = final_kmeans.fit_predict(X_zoo_scaled)\n",
    "pd.crosstab(final_labels, y_zoo, rownames=[\"cluster\"], colnames=[\"type\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster seven matches the real types quite well, so I keep that option for the report.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Agglomerative clustering comparison\n",
    "I repeat the zoo study with the four linkage rules from class and compare external metrics and silhouette.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_methods = [\"single\", \"complete\", \"average\", \"ward\"]\n",
    "agglomerative_rows = []\n",
    "for method in linkage_methods:\n",
    "    model = AgglomerativeClustering(n_clusters=best_k, linkage=method)\n",
    "    labels = model.fit_predict(X_zoo_scaled)\n",
    "    agglomerative_rows.append(\n",
    "        {\n",
    "            \"method\": method,\n",
    "            \"rand\": rand_score(y_zoo, labels),\n",
    "            \"adjusted_rand\": adjusted_rand_score(y_zoo, labels),\n",
    "            \"mutual_info\": mutual_info_score(y_zoo, labels),\n",
    "            \"homogeneity\": homogeneity_score(y_zoo, labels),\n",
    "            \"completeness\": completeness_score(y_zoo, labels),\n",
    "            \"v_measure\": v_measure_score(y_zoo, labels),\n",
    "            \"silhouette\": silhouette_score(X_zoo_scaled, labels),\n",
    "        }\n",
    "    )\n",
    "agglomerative_results = pd.DataFrame(agglomerative_rows)\n",
    "agglomerative_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete linkage keeps the best balance between external scores and silhouette, so I would explain that choice during the presentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DBSCAN manual example\n",
    "The statement shows twelve points in the plane with eps equal to 0.5 and min samples equal to three. I recreate the example with NumPy and scikit learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.array([\n",
    "    [1.0, 1.0],\n",
    "    [1.2, 0.9],\n",
    "    [0.8, 1.1],\n",
    "    [1.0, 1.2],\n",
    "    [8.0, 8.0],\n",
    "    [8.2, 7.9],\n",
    "    [7.9, 8.1],\n",
    "    [8.1, 8.2],\n",
    "    [0.5, 7.5],\n",
    "    [0.6, 7.7],\n",
    "    [0.4, 7.6],\n",
    "    [0.7, 7.4],\n",
    "])\n",
    "\n",
    "model_dbscan = DBSCAN(eps=0.5, min_samples=3)\n",
    "dbscan_labels = model_dbscan.fit_predict(points)\n",
    "\n",
    "dbscan_summary = pd.DataFrame({\n",
    "    \"point\": [f\"P{i+1}\" for i in range(len(points))],\n",
    "    \"x\": points[:, 0],\n",
    "    \"y\": points[:, 1],\n",
    "    \"label\": dbscan_labels,\n",
    "})\n",
    "dbscan_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two groups appear and three points are marked as noise, which matches the picture from class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(points[:, 0], points[:, 1], c=dbscan_labels, cmap=\"tab10\", s=80, edgecolor=\"black\")\n",
    "plt.title(\"DBSCAN result with eps 0.5 and min samples 3\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Image color reduction with K-Means\n",
    "Now I reduce the color palettes of the gradient, stripes, and landscape images from the practice folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path: Path) -> np.ndarray:\n",
    "    image = Image.open(path)\n",
    "    return np.asarray(image, dtype=np.float32) / 255.0\n",
    "\n",
    "\n",
    "def compress_image(array: np.ndarray, k: int, seed: int = 0) -> tuple[np.ndarray, float]:\n",
    "    pixels = array.reshape(-1, array.shape[-1])\n",
    "    model = KMeans(n_clusters=k, random_state=seed, n_init=5)\n",
    "    labels = model.fit_predict(pixels)\n",
    "    palette = model.cluster_centers_\n",
    "    compressed = palette[labels].reshape(array.shape)\n",
    "    mse = float(np.mean((array - compressed) ** 2))\n",
    "    return compressed, mse\n",
    "\n",
    "image_arrays = {\n",
    "    name: load_image(paths[name])\n",
    "    for name in [\"gradient\", \"stripes\", \"landscape\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I test the palette sizes from the statement and record the mean squared error for each case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_plan = {\n",
    "    \"gradient\": [4, 8, 16, 32],\n",
    "    \"stripes\": [3, 5, 7, 9],\n",
    "    \"landscape\": [5, 10, 20, 30],\n",
    "}\n",
    "\n",
    "records = []\n",
    "compressed_examples = {}\n",
    "for name, array in image_arrays.items():\n",
    "    ks = compression_plan[name]\n",
    "    for k in ks:\n",
    "        compressed, mse = compress_image(array, k)\n",
    "        records.append({\"image\": name, \"k\": k, \"mse\": mse})\n",
    "        if k == ks[0]:\n",
    "            compressed_examples[(name, k)] = compressed\n",
    "results_compression = pd.DataFrame(records)\n",
    "results_compression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean squared error goes down when I increase k. The stripes image keeps a tiny error even with small palettes because it already has few colors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(compressed_examples), 2, figsize=(6, 6))\n",
    "for index, ((name, k), compressed) in enumerate(compressed_examples.items()):\n",
    "    original = image_arrays[name]\n",
    "    row_axes = axes[index]\n",
    "    row_axes[0].imshow(original)\n",
    "    row_axes[0].set_title(f\"Original {name}\")\n",
    "    row_axes[0].axis(\"off\")\n",
    "    row_axes[1].imshow(compressed)\n",
    "    row_axes[1].set_title(f\"k = {k}\")\n",
    "    row_axes[1].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PCA with the digits dataset\n",
    "I switch to the digits dataset to practice principal component analysis. I scale the pixels, fit PCA with thirty components, and study the variance ratio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "scaler_digits = StandardScaler()\n",
    "X_digits_scaled = scaler_digits.fit_transform(X_digits)\n",
    "\n",
    "pca = PCA(n_components=30, random_state=0)\n",
    "X_digits_pca = pca.fit_transform(X_digits_scaled)\n",
    "variance_progress = pd.Series(pca.explained_variance_ratio_).cumsum()\n",
    "variance_progress.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first ten components already cover more than eighty percent of the variance, so thirty components are more than enough for the next test.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Classifier comparison with and without PCA\n",
    "I compare logistic regression and k nearest neighbors before and after PCA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(\n",
    "    X_digits_scaled,\n",
    "    y_digits,\n",
    "    test_size=0.25,\n",
    "    random_state=0,\n",
    "    stratify=y_digits,\n",
    ")\n",
    "\n",
    "X_train_pca = pca.transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "logreg = LogisticRegression(max_iter=2000, random_state=0)\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "logreg_pca = LogisticRegression(max_iter=2000, random_state=0)\n",
    "logreg_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "results_classifiers = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"model\": \"LogisticRegression\",\n",
    "            \"dataset\": \"original\",\n",
    "            \"accuracy\": logreg.score(X_test_scaled, y_test),\n",
    "        },\n",
    "        {\n",
    "            \"model\": \"LogisticRegression\",\n",
    "            \"dataset\": \"pca\",\n",
    "            \"accuracy\": logreg_pca.score(X_test_pca, y_test),\n",
    "        },\n",
    "        {\n",
    "            \"model\": \"KNN\",\n",
    "            \"dataset\": \"original\",\n",
    "            \"accuracy\": knn.score(X_test_scaled, y_test),\n",
    "        },\n",
    "        {\n",
    "            \"model\": \"KNN\",\n",
    "            \"dataset\": \"pca\",\n",
    "            \"accuracy\": knn_pca.score(X_test_pca, y_test),\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "results_classifiers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression keeps the best accuracy after PCA, while k nearest neighbors loses a little performance. This matches what we discussed in class about linear models and compression.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}