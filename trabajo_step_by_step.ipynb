    "# Boletin 1 step by step notebook\n",
    "This notebook follows every exercise from the Boletin 1 sheet. The text uses basic English so any new student can read it.\n"
    "## How to run this notebook\n",
    "1. Run each cell from the top. Use the Run button or press Shift Enter.\n",
    "2. Read the short note before a code cell. It explains why we run the code.\n",
    "3. If a cell fails, restart the kernel and start again from the first cell.\n"
    "# Imports used in the whole notebook\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, MiniBatchKMeans\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    adjusted_rand_score,\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from class_helpers import (\n",
    "    ensure_practice_paths,\n",
    "    ensure_faces_dataset,\n",
    "    load_faces,\n",
    "    load_zoo,\n",
    "    scale_features,\n",
    "    grid_search_kmeans,\n",
    "    ensure_output_dir,\n",
    "    paths,\n",
    "    IMAGES_DIR,\n",
    "    OUTPUT_DIR,\n",
    ")\n"
   "execution_count": null,
   "outputs": []
    "# Make sure the teacher resources are ready\n",
    "ensure_practice_paths()\n",
    "print(\"Images folder:\", IMAGES_DIR)\n",
    "print(\"Reduced images folder:\", OUTPUT_DIR)\n",
    "for key in [\"zoo\", \"faces\"]:\n",
    "    print(f\"{key} file:\", paths[key])\n"
   "execution_count": null,
   "outputs": []
    "## Task 1. Zoo dataset and K-Means (without type column)\n",
    "We start with the animals table. The goal is to test K-Means with k = 5, 6, 7, and 8.\n"
    "# Load the zoo data without the type column\n",
    "data_no_type = load_zoo(include_type=False)\n",
    "zoo_table = data_no_type.table\n",
    "X_zoo = data_no_type.features\n",
    "y_zoo = data_no_type.labels\n",
    "print(\"Rows:\", len(zoo_table))\n",
    "zoo_table.head()\n"
   "execution_count": null,
   "outputs": []
    "# Scale the numeric features to help the clustering step\n",
    "print(\"Scaled feature shape:\", X_zoo_scaled.shape)\n"
   ],
   "execution_count": null,
   "outputs": []
    "# Run K-Means for k in {5, 6, 7, 8}\n",
    "results_kmeans = []\n",
    "for k in [5, 6, 7, 8]:\n",
    "    model = KMeans(n_clusters=k, random_state=0, n_init=10)\n",
    "    labels = model.fit_predict(X_zoo_scaled)\n",
    "    inertia = model.inertia_\n",
    "    score = silhouette_score(X_zoo_scaled, labels)\n",
    "    ari = adjusted_rand_score(y_zoo, labels)\n",
    "    results_kmeans.append({\n",
    "        \"k\": k,\n",
    "        \"inertia\": inertia,\n",
    "        \"silhouette\": score,\n",
    "        \"ARI\": ari,\n",
    "    })\n",
    "results_kmeans_df = pd.DataFrame(results_kmeans)\n",
    "results_kmeans_df\n"
   ],
   "execution_count": null,
   "outputs": []
    "The scores show how well each cluster count works. We now plot a 2D view for the best k.\n"
    "# Draw a PCA projection with the chosen k\n",
    "best_k = int(results_kmeans_df.sort_values(\"silhouette\", ascending=False).iloc[0][\"k\"])\n",
    "print(\"Best k by silhouette:\", best_k)\n",
    "model = KMeans(n_clusters=best_k, random_state=0, n_init=10)\n",
    "labels = model.fit_predict(X_zoo_scaled)\n",
    "scatter = plt.scatter(X_zoo_2d[:, 0], X_zoo_2d[:, 1], c=labels, cmap=\"tab10\")\n",
    "plt.title(f\"Zoo animals clustered with K-Means (k={best_k})\")\n",
    "plt.colorbar(scatter, label=\"cluster\")\n",
    "plt.show()\n"
   "execution_count": null,
   "outputs": []
    "## Task 1b. Zoo dataset and K-Means (with type column)\n",
    "We repeat the study but we keep the type column so we can compare the groups.\n"
    "# Load the zoo data with the type column included\n",
    "with_type = load_zoo(include_type=True)\n",
    "X_with_type = with_type.features\n",
    "y_type = with_type.labels\n",
    "scaler_type, X_with_type_scaled = scale_features(X_with_type)\n",
    "print(\"Feature columns:\", list(X_with_type.columns))\n"
   ],
   "execution_count": null,
   "outputs": []
   "cell_type": "code",
    "# Run the same K-Means loop with the extra column\n",
    "results_with_type = []\n",
    "for k in [5, 6, 7, 8]:\n",
    "    model = KMeans(n_clusters=k, random_state=0, n_init=10)\n",
    "    labels = model.fit_predict(X_with_type_scaled)\n",
    "    inertia = model.inertia_\n",
    "    score = silhouette_score(X_with_type_scaled, labels)\n",
    "    ari = adjusted_rand_score(y_type, labels)\n",
    "    results_with_type.append({\n",
    "        \"k\": k,\n",
    "        \"inertia\": inertia,\n",
    "        \"silhouette\": score,\n",
    "        \"ARI\": ari,\n",
    "    })\n",
    "pd.DataFrame(results_with_type)\n"
   ],
   "execution_count": null,
   "outputs": []
    "# Compare one cluster assignment with the real classes\n",
    "model_with_type = KMeans(n_clusters=best_k, random_state=0, n_init=10)\n",
    "labels_with_type = model_with_type.fit_predict(X_with_type_scaled)\n",
    "comparison = pd.crosstab(labels_with_type, y_type, rownames=[\"cluster\"], colnames=[\"type\"])\n",
    "comparison\n"
   ],
   "execution_count": null,
   "outputs": []
    "## Task 2. Hierarchical clustering on the zoo dataset\n",
    "We now apply hierarchical clustering and read the dendrogram to see the groups.\n"
    "# Build a dendrogram with Ward linkage\n",
    "try:\n",
    "    from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "except ImportError:\n",
    "    raise ImportError(\"Install SciPy to draw the dendrogram.\")\n",
    "\n",
    "linkage_matrix = linkage(X_zoo_scaled, method=\"ward\")\n",
    "plt.figure(figsize=(10, 5))\n",
    "dendrogram(linkage_matrix, p=12, truncate_mode=\"lastp\", leaf_rotation=45)\n",
    "plt.title(\"Hierarchical clustering dendrogram (Ward)\")\n",
    "plt.xlabel(\"Cluster index\")\n",
    "plt.ylabel(\"Distance\")\n",
   ],
   "execution_count": null,
   "outputs": []
    "# Try different linkage methods and check their scores\n",
    "for method in [\"ward\", \"average\", \"complete\"]:\n",
    "    model = AgglomerativeClustering(n_clusters=best_k, linkage=method)\n",
   ],
   "execution_count": null,
   "outputs": []
    "# Compare the Ward result with the real classes\n",
    "ward_model = AgglomerativeClustering(n_clusters=best_k, linkage=\"ward\")\n",
   ],
   "execution_count": null,
   "outputs": []
    "## Task 3. DBSCAN toy dataset\n",
    "The sheet gives a small list of 12 two dimensional points. We run DBSCAN with eps = 0.5 and MinPts = 3.\n"
    "# Define the twelve points from the sheet\n",
    "    [0.5, 7.5], [0.6, 7.7], [0.4, 7.6], [0.7, 7.4],\n",
    "])\n",
    "dbscan_points\n"
   ],
   "execution_count": null,
   "outputs": []
    "# Plot the points to see the three groups\n",
    "plt.title(\"Toy dataset for DBSCAN\")\n",
   ],
   "execution_count": null,
   "outputs": []
    "# Run DBSCAN with eps = 0.5 and MinPts = 3\n",
    "print(\"Cluster labels:\", dbscan_labels)\n"
   ],
   "execution_count": null,
   "outputs": []
    "# Inspect the labels next to the coordinates\n",
   ],
   "execution_count": null,
   "outputs": []
    "## Task 4. Image helper utilities\n",
    "We write small helper functions to load, save, and show the images from the teacher pack.\n"
    "# Helper functions for image work\n",
    "    # Return an image as a float array in [0, 1]\n",
    "    with Image.open(path) as img:\n",
    "        array = np.asarray(img, dtype=np.float32) / 255.0\n",
    "    return array\n",
    "    Image.fromarray(clipped).save(path)\n",
    "    return quantized\n",
    "def show_side_by_side(original: np.ndarray, reduced: np.ndarray, title: str) -> None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
   "execution_count": null,
   "outputs": []
    "## Task 5. Image colour reduction\n",
    "We reduce the number of colours for the landscape, gradient, and stripes images.\n"
    "# Quantize each image with the requested palette sizes\n",
    "ensure_output_dir()\n",
    "image_names = [\"landscape\", \"gradient\", \"stripes\"]\n",
    "color_options = [3, 5, 10]\n",
    "for name in image_names:\n",
    "    image_path = IMAGES_DIR / f\"{name}.ppm\"\n",
    "    original = load_image(image_path)\n",
    "    for n_colors in color_options:\n",
    "        reduced = quantize_image(original, n_colors=n_colors, random_state=0)\n",
    "        out_path = OUTPUT_DIR / f\"{name}_k{n_colors}.png\"\n",
    "        save_image(reduced, out_path)\n",
    "        print(f\"Saved {out_path}\")\n"
   "execution_count": null,
   "outputs": []
    "# Show an example comparison\n",
    "sample_name = image_names[0]\n",
    "sample_original = load_image(IMAGES_DIR / f\"{sample_name}.ppm\")\n",
    "sample_reduced = load_image(OUTPUT_DIR / f\"{sample_name}_k{color_options[0]}.png\")\n",
    "show_side_by_side(sample_original, sample_reduced, f\"k = {color_options[0]}\")\n"
   "execution_count": null,
   "outputs": []
    "## Task 6. Image file size study\n",
    "We compare the file sizes before and after the colour reduction.\n"
    "# Collect file sizes for the original and reduced images\n",
    "rows_sizes = []\n",
    "for name in image_names:\n",
    "    original_path = IMAGES_DIR / f\"{name}.ppm\"\n",
    "    rows_sizes.append({\n",
    "        \"image\": name,\n",
    "        \"version\": \"original\",\n",
    "        \"path\": original_path,\n",
    "        \"size_bytes\": original_path.stat().st_size,\n",
    "    })\n",
    "    for n_colors in color_options:\n",
    "        reduced_path = OUTPUT_DIR / f\"{name}_k{n_colors}.png\"\n",
    "        rows_sizes.append({\n",
    "            \"image\": name,\n",
    "            \"version\": f\"k{n_colors}\",\n",
    "            \"path\": reduced_path,\n",
    "            \"size_bytes\": reduced_path.stat().st_size,\n",
    "        })\n",
    "file_sizes = pd.DataFrame(rows_sizes)\n",
    "file_sizes\n"
   ],
   "execution_count": null,
   "outputs": []
    "## Task 7. Prepare the faces dataset\n",
    "The PCA steps need the faces matrix. We build it when missing and load it here.\n"
    "# Make sure faces.mat exists and load it\n",
    "faces_path = paths[\"faces\"]\n",
    "ensure_faces_dataset(faces_path)\n",
    "faces, face_labels = load_faces()\n",
    "print(\"Faces shape:\", faces.shape)\n",
    "print(\"Labels shape:\", face_labels.shape)\n"
   "execution_count": null,
   "outputs": []
    "## Task 8. PCA on faces and explained variance\n",
    "We compress the faces with PCA and rebuild them with different numbers of components.\n"
    "# Try different component counts and record the reconstruction error\n",
    "component_list = [10, 40, 80]\n",
    "faces_float = faces.astype(np.float32)\n",
    "for n in component_list:\n",
    "    pca = PCA(n_components=n, random_state=0)\n",
    "    low_dim = pca.fit_transform(faces_float)\n",
    "    restored = pca.inverse_transform(low_dim)\n",
    "    mse = np.mean((faces_float - restored) ** 2)\n",
    "    explained = pca.explained_variance_ratio_.sum()\n",
    "    print(f\"components={n:3d} -> explained_variance={explained:.3f}, mse={mse:.6f}\")\n"
   ],
   "execution_count": null,
   "outputs": []
    "# Plot the cumulative explained variance curve\n",
    "pca_full = PCA().fit(faces_float)\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "plt.plot(cumulative_variance)\n",
    "plt.title(\"Faces PCA cumulative explained variance\")\n",
    "plt.xlabel(\"number of components\")\n",
    "plt.ylabel(\"cumulative variance\")\n",
   "execution_count": null,
   "outputs": []
    "## Task 9. Classification with digits (before and after PCA)\n",
    "We train logistic regression on the digits dataset. Then we apply PCA and compare the accuracy.\n"
    "# Baseline classifier without PCA\n",
    "digits = load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    digits.data,\n",
    "    digits.target,\n",
    "    test_size=0.25,\n",
    "    random_state=0,\n",
    "    stratify=digits.target,\n",
    ")\n",
    "baseline_model = LogisticRegression(max_iter=2000, random_state=0)\n",
    "baseline_model.fit(X_train, y_train)\n",
    "pred_baseline = baseline_model.predict(X_test)\n",
    "acc_baseline = accuracy_score(y_test, pred_baseline)\n",
    "print(f\"Baseline accuracy: {acc_baseline:.3f}\")\n",
    "print(\"Baseline report:\")\n",
    "print(classification_report(y_test, pred_baseline))\n"
   "execution_count": null,
   "outputs": []
    "# Classifier after PCA\n",
    "pca_digits = PCA(n_components=30, random_state=0)\n",
    "X_train_pca = pca_digits.fit_transform(X_train)\n",
    "X_test_pca = pca_digits.transform(X_test)\n",
    "pca_model = LogisticRegression(max_iter=2000, random_state=0)\n",
    "pca_model.fit(X_train_pca, y_train)\n",
    "pred_pca = pca_model.predict(X_test_pca)\n",
    "acc_pca = accuracy_score(y_test, pred_pca)\n",
    "print(f\"PCA accuracy: {acc_pca:.3f}\")\n",
    "print(\"PCA report:\")\n",
    "print(classification_report(y_test, pred_pca))\n"
   "execution_count": null,
   "outputs": []
    "## Wrap up\n",
    "The notebook now covers every Boletin 1 task with the official teacher images and datasets. Change any parameter and run the cells again to explore.\n"
   "display_name": "Python 3",
   "name": "python3"
   "name": "python"
}