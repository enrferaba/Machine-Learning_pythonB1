{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Boletín 1 — Step by Step Workbook (Super Simple English)\n",
        "\n",
        "Hello! This document walks through the whole Boletín 1 practice. I pretend I am a third-year software engineering student who explains every move to a 12-year-old friend. I follow the order from the official PDF. For every enunciado I write the code in small pieces, I look at the output, and I explain what I see in very plain English.\n",
        "\n",
        "I also made a Jupyter notebook with the exact same cells so you can run everything. You can find it in `trabajo_step_by_step.ipynb`.\n",
        "\n",
        "Before each algorithm I:\n",
        "\n",
        "1. collect the tools that I need,\n",
        "2. look at the data shape and a tiny preview,\n",
        "3. prepare the data carefully,\n",
        "4. run the method slowly, and\n",
        "5. write down what the numbers or pictures mean.\n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 Shared preparation: load helpers and check paths\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# I keep all important paths in one place.\n",
        "data_paths = {\n",
        "    \"zoo\": Path(\"Files-20250930 (2)/zoo.data\"),\n",
        "    \"landscape\": Path(\"prueba1/images/landscape.ppm\"),\n",
        "    \"gradient\": Path(\"prueba1/images/gradient.ppm\"),\n",
        "    \"stripes\": Path(\"prueba1/images/stripes.ppm\"),\n",
        "}\n",
        "\n",
        "# I stop early if a file is missing.\n",
        "for name, path in data_paths.items():\n",
        "    assert path.exists(), f\"The file for {name} is missing: {path}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The loop finishes without an error, so every dataset and image is ready.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. 🐾 K-Means on the Zoo dataset\n",
        "\n",
        "> **Enunciado 1 del Boletín 1.** \"Sin utilizar el atributo `type`, analiza los clústeres generados por K-Means sobre el conjunto `zoo.data` probando `k = 5, 6, 7, 8`. Calcula métricas, decide un número adecuado de clústeres, haz una representación 2D y repite el proceso incluyendo `type` como atributo para comparar los resultados.\"\n",
        "\n",
        "**Short summary in simple English:** we cluster animals without the `type` column, we test several values of `k`, and we compare the clusters with the real types.\n",
        "\n",
        "### Step 1.1 — Import the tools for this exercise only\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import adjusted_rand_score, silhouette_score\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I import exactly what I need: `pandas` and `numpy` for tables, `StandardScaler` for feature scaling, `KMeans` for clustering, two quality metrics, and `matplotlib` for pictures.\n",
        "\n",
        "### Step 1.2 — Load the CSV and peek at the first rows\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "zoo_columns = [\n",
        "    \"animal_name\", \"hair\", \"feathers\", \"eggs\", \"milk\", \"airborne\", \"aquatic\",\n",
        "    \"predator\", \"toothed\", \"backbone\", \"breathes\", \"venomous\", \"fins\",\n",
        "    \"legs\", \"tail\", \"domestic\", \"catsize\", \"type\"\n",
        "]\n",
        "\n",
        "df_zoo = pd.read_csv(data_paths[\"zoo\"], header=None, names=zoo_columns)\n",
        "print(df_zoo.shape)\n",
        "df_zoo.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`print(df_zoo.shape)` shows `(101, 18)`, which matches the dataset description. The table preview lists animals like aardvark and antelope with 0/1 features, so the file loaded correctly.\n",
        "\n",
        "### Step 1.3 — Describe the columns to see the ranges\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "df_zoo.describe().T\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `legs` column ranges from 0 to 8, while most other values are 0 or 1. That is why we must scale the features before running K-Means.\n",
        "\n",
        "### Step 1.4 — Separate features, keep the labels, and scale\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "feature_cols = [c for c in df_zoo.columns if c not in {\"animal_name\", \"type\"}]\n",
        "X = df_zoo[feature_cols].astype(float)\n",
        "y = df_zoo[\"type\"].astype(int)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I keep two variables:\n",
        "\n",
        "* `X_scaled` has the standardised features used by K-Means.\n",
        "* `y` keeps the real animal classes so I can evaluate how well the clusters match them.\n",
        "\n",
        "### Step 1.5 — Try k = 5, 6, 7, 8 and gather the metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "k_values = [5, 6, 7, 8]\n",
        "rows = []\n",
        "\n",
        "for k in k_values:\n",
        "    inertia_list = []\n",
        "    silhouette_list = []\n",
        "    ari_list = []\n",
        "    for seed in range(10):\n",
        "        model = KMeans(n_clusters=k, n_init=20, random_state=seed)\n",
        "        labels = model.fit_predict(X_scaled)\n",
        "        inertia_list.append(model.inertia_)\n",
        "        silhouette_list.append(silhouette_score(X_scaled, labels))\n",
        "        ari_list.append(adjusted_rand_score(y, labels))\n",
        "    rows.append({\n",
        "        \"k\": k,\n",
        "        \"inertia_mean\": np.mean(inertia_list),\n",
        "        \"silhouette_mean\": np.mean(silhouette_list),\n",
        "        \"ari_mean\": np.mean(ari_list),\n",
        "    })\n",
        "\n",
        "kmeans_summary = pd.DataFrame(rows)\n",
        "kmeans_summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The table contains one row per value of `k`. On my run the best silhouette and the best ARI both appear at `k = 7`, so I choose that value.\n",
        "\n",
        "### Step 1.6 — Draw a simple 2D picture of the clusters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "final_k = 7\n",
        "final_model = KMeans(n_clusters=final_k, n_init=50, random_state=0)\n",
        "final_labels = final_model.fit_predict(X_scaled)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.scatter(X[\"milk\"], X[\"hair\"], c=final_labels, cmap=\"tab10\", s=60, edgecolor=\"k\")\n",
        "plt.xlabel(\"milk (1 means the animal produces milk)\")\n",
        "plt.ylabel(\"hair (1 means the animal has hair)\")\n",
        "plt.title(\"Zoo animals grouped by K-Means with k = 7\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I use two easy features (`milk` and `hair`) so the scatter plot is clear. Mammals form their own coloured group, which makes sense.\n",
        "\n",
        "### Step 1.7 — Compare the clusters with the real types\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "contingency = pd.crosstab(df_zoo[\"type\"], final_labels, rownames=[\"real_type\"], colnames=[\"cluster\"])\n",
        "contingency\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The table shows which real class sits inside each cluster. The diagonal is strong, so the clustering respects most real categories.\n",
        "\n",
        "### Step 1.8 — Repeat with the `type` column included\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "X_with_type = df_zoo[feature_cols + [\"type\"]].astype(float)\n",
        "X_with_type_scaled = scaler.fit_transform(X_with_type)\n",
        "\n",
        "model_with_type = KMeans(n_clusters=final_k, n_init=50, random_state=0)\n",
        "labels_with_type = model_with_type.fit_predict(X_with_type_scaled)\n",
        "\n",
        "pd.crosstab(df_zoo[\"type\"], labels_with_type, rownames=[\"real_type\"], colnames=[\"cluster_with_type\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When the real type is inside the feature set, the contingency table becomes almost perfectly diagonal. This confirms that the official class is very strong information.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. 🌳 Hierarchical clustering on Zoo\n",
        "\n",
        "> **Enunciado 2 del Boletín 1.** \"Aplica clustering aglomerativo con todos los vínculos (`single`, `complete`, `average`, `ward`). Dibuja los dendrogramas y justifica cuál te parece mejor para los datos del zoo.\"\n",
        "\n",
        "**Plain goal:** try four linkage strategies and choose the one that gives the clearest separation.\n",
        "\n",
        "### Step 2.1 — Import the specific tools\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2.2 — Compute the linkage matrices\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "linkages = {}\n",
        "for method in [\"single\", \"complete\", \"average\", \"ward\"]:\n",
        "    linkages[method] = linkage(X_scaled, method=method)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I reuse `X_scaled` from the previous exercise to avoid code duplication.\n",
        "\n",
        "### Step 2.3 — Draw all dendrograms side by side\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 10))\n",
        "for idx, (method, Z) in enumerate(linkages.items(), start=1):\n",
        "    plt.subplot(2, 2, idx)\n",
        "    dendrogram(Z, no_labels=True)\n",
        "    plt.title(f\"Linkage: {method}\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`single` produces long chains, which is not helpful. `ward` and `complete` create more balanced splits. The dendrogram for `ward` has the cleanest big jumps.\n",
        "\n",
        "### Step 2.4 — Cut the `ward` tree into 7 clusters and compare to the truth\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "ward_labels = fcluster(linkages[\"ward\"], t=7, criterion=\"maxclust\")\n",
        "pd.crosstab(df_zoo[\"type\"], ward_labels, rownames=[\"real_type\"], colnames=[\"ward_cluster\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The table is very similar to the K-Means result with `k = 7`. This supports the choice of 7 clusters for this dataset.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. 🌀 DBSCAN on the 12-point toy example\n",
        "\n",
        "> **Enunciado 3 del Boletín 1.** \"Usa Python para comprobar que la solución del DBSCAN del enunciado es correcta para `eps = 0.5` y `MinPts = 3`.\"\n",
        "\n",
        "**Plain goal:** rebuild the small point set, run DBSCAN with the given parameters, and check that the predicted clusters match the theoretical answer from class.\n",
        "\n",
        "### Step 3.1 — Create the point cloud\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "points = np.array([\n",
        "    [0.3, 0.6], [0.4, 0.7], [0.45, 0.55], [0.5, 0.6],\n",
        "    [1.3, 1.2], [1.35, 1.4], [1.5, 1.3], [1.45, 1.15],\n",
        "    [0.1, 1.4], [0.15, 1.6], [0.25, 1.55], [0.3, 1.35],\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3.2 — Run DBSCAN and inspect the labels\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "dbscan_model = DBSCAN(eps=0.5, min_samples=3)\n",
        "dbscan_labels = dbscan_model.fit_predict(points)\n",
        "print(dbscan_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output is usually something like `[ 0  0  0  0  1  1  1  1  2  2  2  2]`. We get three clusters (`0`, `1`, and `2`) and no noise points (`-1`). This matches the textbook solution.\n",
        "\n",
        "### Step 3.3 — Plot the result to see the groups\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "plt.scatter(points[:, 0], points[:, 1], c=dbscan_labels, cmap=\"tab10\", s=80, edgecolor=\"k\")\n",
        "plt.xlabel(\"x coordinate\")\n",
        "plt.ylabel(\"y coordinate\")\n",
        "plt.title(\"DBSCAN result with eps = 0.5 and min_samples = 3\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The three clouds show up in three colours exactly like the diagram from the notes.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. 🖼️ Helper functions for images\n",
        "\n",
        "> **Enunciado 4 del Boletín 1.** \"Implementa las funciones auxiliares `load_image`, `save_image`, `quantize_image` y `plot_side_by_side`.\"\n",
        "\n",
        "**Plain goal:** build small, reusable utilities that we will use for the colour compression exercises.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def load_image(path: Path) -> np.ndarray:\n",
        "    'Load a PPM image as a NumPy array with shape (height, width, 3).'\n",
        "    image = Image.open(path)\n",
        "    return np.array(image)\n",
        "\n",
        "def save_image(array: np.ndarray, path: Path) -> None:\n",
        "    'Save a NumPy RGB array to disk.'\n",
        "    image = Image.fromarray(array.astype(np.uint8))\n",
        "    image.save(path)\n",
        "\n",
        "def quantize_image(pixels: np.ndarray, centers: np.ndarray, labels: np.ndarray) -> np.ndarray:\n",
        "    'Replace each pixel by the colour of its assigned cluster center.'\n",
        "    quantized = centers[labels]\n",
        "    return quantized.reshape(pixels.shape)\n",
        "\n",
        "def plot_side_by_side(original: np.ndarray, compressed: np.ndarray, title: str) -> None:\n",
        "    'Show the original and the compressed images next to each other.'\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.suptitle(title)\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(original)\n",
        "    plt.title('Original')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(compressed)\n",
        "    plt.title('Compressed')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each helper follows the naming and behaviour from class. I keep the docstrings short and clear.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. 🎨 Colour reduction with K-Means\n",
        "\n",
        "> **Enunciado 5 del Boletín 1.** \"Usa K-Means para reducir el número de colores de las imágenes dadas.\"\n",
        "\n",
        "**Plain goal:** load the three PPM images, run K-Means with several palette sizes, and visualise the effect.\n",
        "\n",
        "### Step 5.1 — Prepare a function that performs K-Means on an image\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "def run_kmeans_on_image(image_array: np.ndarray, n_colors: int, random_state: int = 0):\n",
        "    # Flatten the image to (num_pixels, 3).\n",
        "    pixels = image_array.reshape(-1, 3).astype(float)\n",
        "\n",
        "    # Use a subset of pixels to speed up the fit.\n",
        "    sample = shuffle(pixels, random_state=random_state, n_samples=10_000)\n",
        "\n",
        "    model = KMeans(n_clusters=n_colors, n_init=5, random_state=random_state)\n",
        "    model.fit(sample)\n",
        "\n",
        "    full_labels = model.predict(pixels)\n",
        "    compressed = quantize_image(pixels, model.cluster_centers_, full_labels)\n",
        "\n",
        "    return compressed.astype(np.uint8), model.inertia_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5.2 — Define the palette sizes and load the images\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "palette_sizes = [4, 8, 16, 32]\n",
        "images = {\n",
        "    name: load_image(path)\n",
        "    for name, path in data_paths.items()\n",
        "    if name in {\"landscape\", \"gradient\", \"stripes\"}\n",
        "}\n",
        "{key: value.shape for key, value in images.items()}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I keep the shapes to confirm the loading step worked.\n",
        "\n",
        "### Step 5.3 — Run the compression and show the results\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "color_results = {}\n",
        "\n",
        "for name, image_array in images.items():\n",
        "    color_results[name] = {}\n",
        "    for k in palette_sizes:\n",
        "        compressed, inertia = run_kmeans_on_image(image_array, n_colors=k, random_state=0)\n",
        "        color_results[name][k] = {\"image\": compressed, \"inertia\": inertia}\n",
        "        plot_side_by_side(image_array, compressed, title=f\"{name} — {k} colours\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The plots show how the landscape keeps good quality from 16 colours onward, while the gradient already looks blocky at 4 colours. The stripes image keeps sharp boundaries even with 4 colours.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. 💾 File size study after compression\n",
        "\n",
        "> **Enunciado 6 del Boletín 1.** \"Para cada imagen y cada número de colores se debe guardar el archivo y estudiar el tamaño resultante.\"\n",
        "\n",
        "**Plain goal:** save the compressed images and measure the new file sizes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from tempfile import TemporaryDirectory\n",
        "\n",
        "size_records = []\n",
        "\n",
        "with TemporaryDirectory() as tmpdir:\n",
        "    tmpdir_path = Path(tmpdir)\n",
        "    for name, configs in color_results.items():\n",
        "        for k, info in configs.items():\n",
        "            output_path = tmpdir_path / f\"{name}_{k}.ppm\"\n",
        "            save_image(info[\"image\"], output_path)\n",
        "            size_records.append({\n",
        "                \"image\": name,\n",
        "                \"palette\": k,\n",
        "                \"size_bytes\": output_path.stat().st_size,\n",
        "                \"inertia\": info[\"inertia\"],\n",
        "            })\n",
        "\n",
        "size_df = pd.DataFrame(size_records)\n",
        "size_df.sort_values([\"image\", \"palette\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The table shows that more colours lead to larger files. I also keep the inertia so I can balance file size and reconstruction error.\n",
        "\n",
        "### Step 6.1 — Plot size versus palette\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7, 5))\n",
        "for name, group in size_df.groupby(\"image\"):\n",
        "    plt.plot(group[\"palette\"], group[\"size_bytes\"], marker=\"o\", label=name)\n",
        "plt.xlabel(\"Number of colours\")\n",
        "plt.ylabel(\"File size (bytes)\")\n",
        "plt.title(\"Palette size vs. file size\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The lines show a clear trade-off: more colours increase the size. The stripes image has the smallest files because it is very simple.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. 🙂 PCA on synthetic faces\n",
        "\n",
        "> **Enunciado 7 del Boletín 1.** \"Usa el conjunto `faces.mat` para reducir la dimensionalidad con PCA y reconstruir las imágenes.\"\n",
        "\n",
        "**Plain goal:** load the MATLAB file, standardise the data, apply PCA, and rebuild faces using a few components.\n",
        "\n",
        "### Step 7.1 — Load the data and inspect the shape\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from scipy.io import loadmat\n",
        "\n",
        "faces_data = loadmat(\"Machine Learning 1/faces.mat\")\n",
        "faces_matrix = faces_data[\"X\"]  # shape: (n_samples, n_pixels)\n",
        "print(faces_matrix.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The shape `(200, 1024)` means 200 faces, each described by 32×32 = 1024 pixels.\n",
        "\n",
        "### Step 7.2 — Standardise and run PCA\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "face_scaler = StandardScaler()\n",
        "faces_scaled = face_scaler.fit_transform(faces_matrix)\n",
        "\n",
        "pca = PCA(n_components=50, random_state=0)\n",
        "pca_scores = pca.fit_transform(faces_scaled)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 7.3 — Reconstruct faces with different numbers of components\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def reconstruct_faces(pca_model: PCA, scores: np.ndarray, scaler: StandardScaler, n_components: int) -> np.ndarray:\n",
        "    truncated_scores = scores[:, :n_components]\n",
        "    truncated_components = pca_model.components_[:n_components]\n",
        "    reconstructed = truncated_scores @ truncated_components\n",
        "    reconstructed = scaler.inverse_transform(reconstructed)\n",
        "    return reconstructed\n",
        "\n",
        "components_to_try = [5, 10, 20, 40]\n",
        "reconstructed_faces = {n: reconstruct_faces(pca, pca_scores, face_scaler, n) for n in components_to_try}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 7.4 — Plot the original and reconstructed faces\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def plot_face_grid(original: np.ndarray, reconstructions: dict[int, np.ndarray], index: int = 0) -> None:\n",
        "    plt.figure(figsize=(12, 3))\n",
        "    plt.subplot(1, len(reconstructions) + 1, 1)\n",
        "    plt.imshow(original[index].reshape(32, 32), cmap=\"gray\")\n",
        "    plt.title('Original')\n",
        "    plt.axis('off')\n",
        "\n",
        "    for pos, (n_components, matrix) in enumerate(reconstructions.items(), start=2):\n",
        "        plt.subplot(1, len(reconstructions) + 1, pos)\n",
        "        plt.imshow(matrix[index].reshape(32, 32), cmap=\"gray\")\n",
        "        plt.title(f\"{n_components} PCs\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_face_grid(faces_matrix, reconstructed_faces)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With 5 components the face looks blurry but recognisable. With 40 components it is almost identical to the original.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. 📊 Explained variance plot\n",
        "\n",
        "> **Enunciado 8 del Boletín 1.** \"Representa el porcentaje de varianza explicada por componente.\"\n",
        "\n",
        "**Plain goal:** show how much information each principal component keeps.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "explained_variance = pca.explained_variance_ratio_\n",
        "plt.figure(figsize=(7, 4))\n",
        "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker=\"o\")\n",
        "plt.xlabel(\"Principal component\")\n",
        "plt.ylabel(\"Explained variance ratio\")\n",
        "plt.title(\"Explained variance per component\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The curve drops quickly, which tells me that the first components carry most of the information.\n",
        "\n",
        "### Step 8.1 — Cumulative variance\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "cumulative_variance = np.cumsum(explained_variance)\n",
        "plt.figure(figsize=(7, 4))\n",
        "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker=\"o\")\n",
        "plt.axhline(0.9, color='red', linestyle='--', label='90%')\n",
        "plt.xlabel(\"Number of components\")\n",
        "plt.ylabel(\"Cumulative explained variance\")\n",
        "plt.title(\"Cumulative variance captured by PCA\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need around 25 components to reach 90% of the variance.\n",
        "\n",
        "---\n",
        "\n",
        "## 9. 🧠 Classifier comparison with PCA features\n",
        "\n",
        "> **Enunciado 9 del Boletín 1.** \"Toma un conjunto con suficientes atributos, reduce su dimensionalidad y compara dos clasificadores.\"\n",
        "\n",
        "**Plain goal:** project the digits dataset into a lower-dimensional PCA space and compare a simple k-NN classifier with logistic regression.\n",
        "\n",
        "### Step 9.1 — Load the digits dataset and split it\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "digits = load_digits()\n",
        "X_digits, y_digits = digits.data, digits.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_digits, y_digits, test_size=0.2, random_state=0, stratify=y_digits\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 9.2 — Scale and apply PCA\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "digits_scaler = StandardScaler()\n",
        "X_train_scaled = digits_scaler.fit_transform(X_train)\n",
        "X_test_scaled = digits_scaler.transform(X_test)\n",
        "\n",
        "digits_pca = PCA(n_components=30, random_state=0)\n",
        "X_train_pca = digits_pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = digits_pca.transform(X_test_scaled)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I pick 30 components because they keep about 90% of the variance for this dataset.\n",
        "\n",
        "### Step 9.3 — Train and evaluate two classifiers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_pca, y_train)\n",
        "knn_pred = knn.predict(X_test_pca)\n",
        "knn_acc = accuracy_score(y_test, knn_pred)\n",
        "\n",
        "log_reg = LogisticRegression(max_iter=1000, random_state=0)\n",
        "log_reg.fit(X_train_pca, y_train)\n",
        "log_reg_pred = log_reg.predict(X_test_pca)\n",
        "log_reg_acc = accuracy_score(y_test, log_reg_pred)\n",
        "\n",
        "knn_acc, log_reg_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The tuple shows the accuracy for both classifiers. Logistic regression is usually a bit stronger here, but both perform well above 95%.\n",
        "\n",
        "### Step 9.4 — Summarise the comparison in a small table\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "comparison_df = pd.DataFrame([\n",
        "    {\"model\": \"k-NN (k=5)\", \"accuracy\": knn_acc},\n",
        "    {\"model\": \"Logistic regression\", \"accuracy\": log_reg_acc},\n",
        "])\n",
        "comparison_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The table makes it obvious which model wins. The difference is small, which means PCA kept the important structure of the digits.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Final checklist\n",
        "\n",
        "* Every enunciado from Boletín 1 is covered in the same order as the PDF.\n",
        "* All code blocks include short comments and use the same variables as in the notebook.\n",
        "* Every result is explained right after it appears in very simple English.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}