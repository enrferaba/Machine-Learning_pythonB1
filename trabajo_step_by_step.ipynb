  "cells": [
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Boletin 1 step by step notebook\n",
        "\n",
        "This notebook follows the class work with the new image pack. The text uses simple English so any new student can follow it."
      ]
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Prepare folders and tools\n",
        "\n",
        "Goal: confirm that the helper files and images are ready before we start. Run the next cell every time you open the notebook. If something is missing the helper will explain what to do."
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from class_helpers import (\n",
        "    ensure_practice_paths,\n",
        "    load_zoo,\n",
        "    scale_features,\n",
        "    grid_search_kmeans,\n",
        "    load_image_array,\n",
        "    ensure_output_dir,\n",
        "    load_faces,\n",
        "    load_digits_split,\n",
        "    IMAGES_DIR,\n",
        "    OUTPUT_DIR,\n",
        ")\n",
        "\n",
        "print(\"Data folder:\", IMAGES_DIR.parent)\n",
        "print(\"Images folder:\", IMAGES_DIR)\n",
        "print(\"Reduced images folder:\", OUTPUT_DIR)"
      "metadata": {},
      "source": [
        "The helper created every folder. Now I check which images are present before any transformation."
      ]
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "image_paths = sorted(IMAGES_DIR.glob(\"*.ppm\"))\n",
        "for path in image_paths:\n",
        "    size = path.stat().st_size\n",
        "    print(f\"- {path.name} ({size} bytes)\")\n",
        "\n",
        "if not image_paths:\n",
        "    print(\"No images were found. Open Files-20250930 (2)/prepare_official_files.ipynb and rebuild the pack.\")"
      "source": [
        "## 1. Zoo dataset warm up\n",
        "\n",
        "Goal: load the table, explore the columns, and prepare the numeric data for clustering."
        "### 1.1 Load the CSV file\n",
        "\n",
        "I use the helper function to read `zoo.data`. The result contains the original table, the list of feature names, the feature matrix, and the labels."
      "execution_count": null,
      "outputs": [],
      "source": [
        "zoo_data = load_zoo()\n",
        "print(\"Rows:\", len(zoo_data.table))\n",
        "print(\"Columns:\", list(zoo_data.table.columns))\n",
        "zoo_data.table.head()"
        "### 1.2 Scale the features\n",
        "\n",
        "Clustering works better when every feature has similar scale. I standardize the numeric columns with the helper function."
        "scaler, zoo_scaled = scale_features(zoo_data.features)\n",
        "print(\"Scaled shape:\", zoo_scaled.shape)\n",
        "print(\"Mean per column (approx zero):\", np.round(zoo_scaled.mean(axis=0), 3))\n",
        "print(\"Std per column (approx one):\", np.round(zoo_scaled.std(axis=0), 3))"
      "source": [
        "### 1.3 First K-Means try\n",
        "\n",
        "I start with three clusters because the data has mammals, birds, and other animals. The cell prints the inertia score and shows how many animals go to each group."
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans_basic = KMeans(n_clusters=3, random_state=0, n_init=10)\n",
        "zoo_groups = kmeans_basic.fit_predict(zoo_scaled)\n",
        "print(\"Inertia:\", round(kmeans_basic.inertia_, 2))\n",
        "unique, counts = np.unique(zoo_groups, return_counts=True)\n",
        "for label, count in zip(unique, counts):\n",
        "    print(f\"Cluster {label}: {count} animals\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4 Compare different K values\n",
        "\n",
        "Now I try several cluster counts and seeds. The helper builds a table with inertia, silhouette, and Adjusted Rand Index. A higher silhouette and ARI mean better clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "k_values = [2, 3, 4, 5]\n",
        "seed_values = [0, 1, 2]\n",
        "\n",
        "kmeans_report = grid_search_kmeans(zoo_scaled, zoo_data.labels, k_values, seed_values)\n",
        "kmeans_report.sort_values([\"silhouette\", \"ARI\"], ascending=False).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Hierarchical clustering\n",
        "\n",
        "Goal: repeat the zoo study with a different method. I compare the three most common linkage rules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "linkage_choices = [\"ward\", \"average\", \"complete\"]\n",
        "for linkage in linkage_choices:\n",
        "    model = AgglomerativeClustering(n_clusters=3, linkage=linkage)\n",
        "    labels = model.fit_predict(zoo_scaled)\n",
        "    sil = silhouette_score(zoo_scaled, labels)\n",
        "    print(f\"Linkage {linkage}: silhouette {sil:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. DBSCAN experiment\n",
        "\n",
        "Goal: test a density based model. I keep the code short and change only the `eps` radius."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "eps_values = [0.3, 0.4, 0.5]\n",
        "for eps in eps_values:\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=3)\n",
        "    labels = dbscan.fit_predict(zoo_scaled)\n",
        "    clusters = [label for label in np.unique(labels) if label != -1]\n",
        "    noise = np.sum(labels == -1)\n",
        "    print(f\"eps {eps}: clusters {clusters} | noise points {noise}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Reduce image colors with K-Means\n",
        "\n",
        "Goal: load each PPM image, run color reduction, and save the result inside `files/reduced_images`. The output files use PNG so they are easy to view."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "ensure_output_dir()\n",
        "\n",
        "image_names = [\"landscape\", \"gradient\", \"stripes\"]\n",
        "cluster_counts = [3, 5, 10]\n",
        "\n",
        "for name in image_names:\n",
        "    original = load_image_array(name)\n",
        "    h, w, c = original.shape\n",
        "    pixels = original.reshape(-1, c)\n",
        "    print(f\"Working on {name} ({h} x {w})\")\n",
        "    for k in cluster_counts:\n",
        "        model = KMeans(n_clusters=k, random_state=0, n_init=10)\n",
        "        labels = model.fit_predict(pixels)\n",
        "        palette = model.cluster_centers_\n",
        "        reduced = palette[labels].reshape(h, w, c)\n",
        "        save_path = OUTPUT_DIR / f\"{name}_k{k}.png\"\n",
        "        plt.imsave(save_path, reduced)\n",
        "        print(f\"  Saved {save_path.name}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Face dataset and PCA\n",
        "\n",
        "Goal: rebuild the class example that projects face images to a small number of principal components. If the dataset is missing the helper prints a clear message."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "try:\n",
        "    faces_X, faces_y = load_faces()\n",
        "except Exception as error:\n",
        "    print(\"Could not load faces dataset:\", error)\n",
        "else:\n",
        "    print(\"Faces shape:\", faces_X.shape)\n",
        "    pca = PCA(n_components=25, random_state=0)\n",
        "    faces_low = pca.fit_transform(faces_X)\n",
        "    faces_back = pca.inverse_transform(faces_low)\n",
        "    print(\"Explained variance ratio:\", np.round(pca.explained_variance_ratio_.sum(), 3))\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(9, 3))\n",
        "    for ax, data, title in zip(\n",
        "        axes,\n",
        "        [faces_X[0], faces_back[0], np.abs(faces_X[0] - faces_back[0])],\n",
        "        [\"Original\", \"Rebuild\", \"Absolute diff\"],\n",
        "    ):\n",
        "        ax.imshow(data.reshape(64, 64), cmap=\"gray\")\n",
        "        ax.set_title(title)\n",
        "        ax.axis(\"off\")\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Digits classification quick test\n",
        "\n",
        "Goal: train a simple logistic regression model on the digits dataset and report the accuracy score on the test split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X_train, X_test, y_train, y_test = load_digits_split(test_size=0.3, random_state=0)\n",
        "\n",
        "scaler_digits = StandardScaler()\n",
        "X_train_scaled = scaler_digits.fit_transform(X_train)\n",
        "X_test_scaled = scaler_digits.transform(X_test)\n",
        "\n",
        "clf = LogisticRegression(max_iter=200, multi_class=\"auto\")\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Wrap up\n",
        "\n",
        "You can now repeat any module. Change the parameters, re run the cells, and take notes about how the scores move. Save the notebook when you finish so you can show the full path during the evaluation."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5