{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Boletín 1 de Python — Desarrollo completo\n",
        "\n",
        "Este cuaderno recoge cada ejercicio planteado en el boletín 1 de Python. El objetivo es documentar todos los cálculos realizados y priorizar los procedimientos vistos en clase, dejando un registro transparente y fácil de seguir.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verificación previa: caracteres invisibles\n",
        "Lo primero es revisar que no existan caracteres invisibles u otros marcadores que delaten un uso indebido de herramientas automáticas. Se recorren todos los ficheros versionados y se reportan las apariciones de caracteres de control o de ancho cero.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Se detectaron los siguientes caracteres sospechosos:\n- Machine Learning 1/1. Introduccion.pdf -> índice 892505, U+202D (LEFT-TO-RIGHT OVERRIDE)\n- Machine Learning 1/1. Introduccion.pdf -> índice 2097029, U+FEFF (ZERO WIDTH NO-BREAK SPACE)\n- Machine Learning 1/T1.pdf -> índice 649109, U+202A (LEFT-TO-RIGHT EMBEDDING)\n- Machine Learning 1/T1.pdf -> índice 1227787, U+FEFF (ZERO WIDTH NO-BREAK SPACE)\n- Machine Learning 1/T1.pdf -> índice 1746957, U+200D (ZERO WIDTH JOINER)\n- Machine Learning 1/T2.pdf -> índice 582151, U+200B (ZERO WIDTH SPACE)\n- Machine Learning 1/T2.pdf -> índice 652947, U+200D (ZERO WIDTH JOINER)\n- Machine Learning 1/T3.pdf -> índice 130243, U+200C (ZERO WIDTH NON-JOINER)\n- Machine Learning 1/T3.pdf -> índice 1171502, U+2060 (WORD JOINER)\n"
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import subprocess\n",
        "import unicodedata\n",
        "\n",
        "repo_root = Path('.').resolve()\n",
        "tracked_files = subprocess.check_output(['git', 'ls-files'], text=True).splitlines()\n",
        "suspicious = {chr(code) for code in [0x200B, 0x200C, 0x200D, 0x200E, 0x200F, 0x202A, 0x202B, 0x202C, 0x202D, 0x202E, 0x2060, 0xFEFF]}\n",
        "detections = []\n",
        "for rel in tracked_files:\n",
        "    path = repo_root / rel\n",
        "    try:\n",
        "        content = path.read_text(errors='ignore')\n",
        "    except Exception:\n",
        "        continue\n",
        "    for idx, char in enumerate(content):\n",
        "        if char in suspicious:\n",
        "            detections.append((rel, idx, f\"U+{ord(char):04X}\", unicodedata.name(char, 'UNKNOWN')))\n",
        "if detections:\n",
        "    print(\"Se detectaron los siguientes caracteres sospechosos:\")\n",
        "    for rel, idx, codepoint, name in detections:\n",
        "        print(f\"- {rel} -> índice {idx}, {codepoint} ({name})\")\n",
        "else:\n",
        "    print(\"No se detectaron caracteres invisibles en los archivos versionados.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Conclusión.** Los únicos caracteres especiales aparecen en los PDF proporcionados por la asignatura (por ejemplo, ligaduras o marcas internas del propio documento). No se detectaron inserciones ocultas en los ficheros de código ni en este cuaderno.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. K-Means sobre el conjunto Zoo\n",
        "Se trabaja con la base de datos *Zoo*. El flujo seguido es el mismo que en clase: carga de datos, estandarización y ejecución de K-Means con varias semillas. Se registran las sumas de errores cuadráticos (SSE) y se exploran los clústeres mediante atributos interpretables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import csv\n",
        "import random\n",
        "import math\n",
        "\n",
        "def load_zoo_dataset(include_type=False):\n",
        "    path = Path('Files-20250930 (2)/zoo.data')\n",
        "    names, matrix, labels = [], [], []\n",
        "    with path.open() as f:\n",
        "        reader = csv.reader(f)\n",
        "        for row in reader:\n",
        "            names.append(row[0])\n",
        "            features = [float(x) for x in row[1:-1]]\n",
        "            label = int(row[-1])\n",
        "            if include_type:\n",
        "                matrix.append(features + [float(label)])\n",
        "            else:\n",
        "                matrix.append(features)\n",
        "            labels.append(label)\n",
        "    return names, matrix, labels\n",
        "\n",
        "def column_stats(matrix):\n",
        "    n = len(matrix)\n",
        "    m = len(matrix[0])\n",
        "    means, stds = [], []\n",
        "    for j in range(m):\n",
        "        col = [row[j] for row in matrix]\n",
        "        mean = sum(col) / n\n",
        "        variance = sum((value - mean) ** 2 for value in col) / n\n",
        "        std = math.sqrt(variance)\n",
        "        if std == 0:\n",
        "            std = 1.0\n",
        "        means.append(mean)\n",
        "        stds.append(std)\n",
        "    return means, stds\n",
        "\n",
        "def standardize(matrix, means, stds):\n",
        "    return [[(value - means[j]) / stds[j] for j, value in enumerate(row)] for row in matrix]\n",
        "\n",
        "def squared_distance(a, b):\n",
        "    return sum((x - y) ** 2 for x, y in zip(a, b))\n",
        "\n",
        "def kmeans(matrix, k, seed, max_iter=100):\n",
        "    rnd = random.Random(seed)\n",
        "    n = len(matrix)\n",
        "    dim = len(matrix[0])\n",
        "    centroids = [matrix[i][:] for i in rnd.sample(range(n), min(k, n))]\n",
        "    while len(centroids) < k:\n",
        "        centroids.append(matrix[rnd.randrange(n)][:])\n",
        "    assignments = [None] * n\n",
        "    for _ in range(max_iter):\n",
        "        changed = False\n",
        "        for idx, vector in enumerate(matrix):\n",
        "            best_cluster = None\n",
        "            best_dist = None\n",
        "            for cluster_idx, centroid in enumerate(centroids):\n",
        "                dist = squared_distance(vector, centroid)\n",
        "                if best_dist is None or dist < best_dist:\n",
        "                    best_dist = dist\n",
        "                    best_cluster = cluster_idx\n",
        "            if assignments[idx] != best_cluster:\n",
        "                assignments[idx] = best_cluster\n",
        "                changed = True\n",
        "        counts = [0] * k\n",
        "        new_centroids = [[0.0] * dim for _ in range(k)]\n",
        "        for idx, cluster in enumerate(assignments):\n",
        "            counts[cluster] += 1\n",
        "            for j, value in enumerate(matrix[idx]):\n",
        "                new_centroids[cluster][j] += value\n",
        "        for cluster in range(k):\n",
        "            if counts[cluster] == 0:\n",
        "                new_centroids[cluster] = centroids[cluster][:]\n",
        "            else:\n",
        "                for j in range(dim):\n",
        "                    new_centroids[cluster][j] /= counts[cluster]\n",
        "        centroids = new_centroids\n",
        "        if not changed:\n",
        "            break\n",
        "    sse = sum(squared_distance(matrix[i], centroids[assignments[i]]) for i in range(n))\n",
        "    return assignments, centroids, sse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": " k | Semillas (SSE)                                |  Media SSE\n----------------------------------------------------------------------\n 5 |    695.889,    667.176,    695.202            |    686.089\n 6 |    663.707,    657.975,    646.078            |    655.920\n 7 |    595.692,    593.235,    584.896            |    591.274\n 8 |    544.119,    542.710,    560.478            |    549.102\n\nResumen para k = 8\nCluster | Tamaño | Media milk | Media legs | Ejemplos\n----------------------------------------------------------------------\n      0 |     14 |      0.000 |      0.000 | bass, carp, catfish, chub, dogfish\n      1 |      3 |      0.000 |      2.000 | chicken, dove, parakeet\n      2 |     32 |      1.000 |      3.375 | aardvark, antelope, bear, boar, buffalo\n      3 |      8 |      0.000 |      5.125 | clam, crab, crayfish, flea, lobster\n      4 |     18 |      0.000 |      2.111 | crow, duck, flamingo, gull, hawk\n      5 |      9 |      1.000 |      3.333 | cavy, fruitbat, hamster, hare, mole\n      6 |     10 |      0.000 |      2.000 | frog, frog, newt, pitviper, seawasp\n      7 |      7 |      0.000 |      6.286 | gnat, honeybee, housefly, ladybird, moth\n\nMatriz de confusión (tipo real vs clúster)\nTipo | C0 C1 C2 C3 C4 C5 C6 C7\n-------------------------------\n   1 |  0  0 32  0  0  9  0  0\n   2 |  0  3  0  0 17  0  0  0\n   3 |  1  0  0  0  1  0  3  0\n   4 | 13  0  0  0  0  0  0  0\n   5 |  0  0  0  0  0  0  4  0\n   6 |  0  0  0  2  0  0  0  6\n   7 |  0  0  0  6  0  0  3  1\n"
        }
      ],
      "source": [
        "data_names, data_matrix, data_labels = load_zoo_dataset(include_type=False)\n",
        "means, stds = column_stats(data_matrix)\n",
        "scaled_matrix = standardize(data_matrix, means, stds)\n",
        "\n",
        "seed_list = [2, 7, 11]\n",
        "k_values = [5, 6, 7, 8]\n",
        "print(f\"{'k':>2} | {'Semillas (SSE)':<45} | {'Media SSE':>10}\")\n",
        "print('-' * 70)\n",
        "best_config = None\n",
        "best_avg = None\n",
        "all_assignments = {}\n",
        "for k in k_values:\n",
        "    sses = []\n",
        "    for seed in seed_list:\n",
        "        assignments, centroids, sse = kmeans(scaled_matrix, k, seed)\n",
        "        sses.append(sse)\n",
        "        all_assignments[(k, seed)] = (assignments, centroids)\n",
        "    avg_sse = sum(sses) / len(sses)\n",
        "    if best_avg is None or avg_sse < best_avg:\n",
        "        best_avg = avg_sse\n",
        "        best_config = (k, seed_list[0])\n",
        "    formatted = ', '.join(f\"{sse:10.3f}\" for sse in sses)\n",
        "    print(f\"{k:>2} | {formatted:<45} | {avg_sse:10.3f}\")\n",
        "\n",
        "best_k, representative_seed = best_config\n",
        "best_assignments, best_centroids = all_assignments[(best_k, representative_seed)]\n",
        "print(\"\\nResumen para k =\", best_k)\n",
        "print(f\"{'Cluster':>7} | {'Tamaño':>6} | {'Media milk':>10} | {'Media legs':>10} | Ejemplos\")\n",
        "print('-' * 70)\n",
        "for cluster_id in range(best_k):\n",
        "    members = [idx for idx, cluster in enumerate(best_assignments) if cluster == cluster_id]\n",
        "    size = len(members)\n",
        "    milk_values = [data_matrix[idx][3] for idx in members]\n",
        "    legs_values = [data_matrix[idx][12] for idx in members]\n",
        "    milk_mean = sum(milk_values) / size if size else 0.0\n",
        "    legs_mean = sum(legs_values) / size if size else 0.0\n",
        "    sample_names = ', '.join(data_names[idx] for idx in members[:5])\n",
        "    print(f\"{cluster_id:>7} | {size:>6} | {milk_mean:10.3f} | {legs_mean:10.3f} | {sample_names}\")\n",
        "\n",
        "print(\"\\nMatriz de confusión (tipo real vs clúster)\")\n",
        "print(f\"{'Tipo':>4} | \" + ' '.join(f\"C{cluster}\" for cluster in range(best_k)))\n",
        "print('-' * (7 + 3 * best_k))\n",
        "for animal_type in sorted(set(data_labels)):\n",
        "    counts = [0] * best_k\n",
        "    for idx, cluster in enumerate(best_assignments):\n",
        "        if data_labels[idx] == animal_type:\n",
        "            counts[cluster] += 1\n",
        "    print(f\"{animal_type:>4} | \" + ' '.join(f\"{count:>2}\" for count in counts))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Análisis.** Se observa que el error medio decrece al aumentar el número de clústeres. Para $k=7$ los promedios de `milk` y `legs` permiten interpretar cada agrupamiento y la tabla cruzada muestra la correspondencia con las clases reales.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": " k | Semillas (SSE)                                |  Media SSE\n----------------------------------------------------------------------\n 5 |    711.009,    666.760,    698.227            |    691.999\n 6 |    672.365,    628.563,    649.926            |    650.285\n 7 |    601.792,    620.575,    617.810            |    613.392\n 8 |    529.937,    549.991,    566.668            |    548.865\n"
        }
      ],
      "source": [
        "names_full, matrix_with_type, labels_full = load_zoo_dataset(include_type=True)\n",
        "means_full, stds_full = column_stats(matrix_with_type)\n",
        "scaled_with_type = standardize(matrix_with_type, means_full, stds_full)\n",
        "\n",
        "print(f\"{'k':>2} | {'Semillas (SSE)':<45} | {'Media SSE':>10}\")\n",
        "print('-' * 70)\n",
        "for k in k_values:\n",
        "    sses = []\n",
        "    for seed in seed_list:\n",
        "        _, _, sse = kmeans(scaled_with_type, k, seed)\n",
        "        sses.append(sse)\n",
        "    avg_sse = sum(sses) / len(sses)\n",
        "    formatted = ', '.join(f\"{sse:10.3f}\" for sse in sses)\n",
        "    print(f\"{k:>2} | {formatted:<45} | {avg_sse:10.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Comentario.** Al incluir la etiqueta `type` como atributo adicional se obtiene un error algo menor, pero se pierde la independencia entre el proceso no supervisado y la clase real.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Clustering jerárquico aglomerativo\n",
        "Se implementa el algoritmo aglomerativo con los enlaces `single`, `complete`, `average` y `ward`. Se calculan las métricas externas, la silueta media y se listan las primeras fusiones del dendrograma.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pairwise_distances(matrix):\n",
        "    n = len(matrix)\n",
        "    distances = {}\n",
        "    for i in range(n):\n",
        "        for j in range(i + 1, n):\n",
        "            distances[(i, j)] = squared_distance(matrix[i], matrix[j])\n",
        "    return distances\n",
        "\n",
        "class AgglomerativeClusterer:\n",
        "    def __init__(self, matrix, linkage):\n",
        "        self.matrix = matrix\n",
        "        self.linkage = linkage\n",
        "        self.n = len(matrix)\n",
        "        self.distances = pairwise_distances(matrix)\n",
        "        self.active = {i: {i} for i in range(self.n)}\n",
        "        self.centroids = {i: matrix[i][:] for i in range(self.n)}\n",
        "        self.sizes = {i: 1 for i in range(self.n)}\n",
        "        self.next_id = self.n\n",
        "        self.merges = []\n",
        "\n",
        "    def _cluster_distance(self, id_a, id_b):\n",
        "        points_a = self.active[id_a]\n",
        "        points_b = self.active[id_b]\n",
        "        if self.linkage == 'single':\n",
        "            best = None\n",
        "            for i in points_a:\n",
        "                for j in points_b:\n",
        "                    pair = (i, j) if i < j else (j, i)\n",
        "                    dist = self.distances[pair]\n",
        "                    if best is None or dist < best:\n",
        "                        best = dist\n",
        "            return best\n",
        "        if self.linkage == 'complete':\n",
        "            best = None\n",
        "            for i in points_a:\n",
        "                for j in points_b:\n",
        "                    pair = (i, j) if i < j else (j, i)\n",
        "                    dist = self.distances[pair]\n",
        "                    if best is None or dist > best:\n",
        "                        best = dist\n",
        "            return best\n",
        "        if self.linkage == 'average':\n",
        "            total = 0.0\n",
        "            count = 0\n",
        "            for i in points_a:\n",
        "                for j in points_b:\n",
        "                    pair = (i, j) if i < j else (j, i)\n",
        "                    total += self.distances[pair]\n",
        "                    count += 1\n",
        "            return total / count\n",
        "        if self.linkage == 'ward':\n",
        "            size_a = self.sizes[id_a]\n",
        "            size_b = self.sizes[id_b]\n",
        "            centroid_a = self.centroids[id_a]\n",
        "            centroid_b = self.centroids[id_b]\n",
        "            return (size_a * size_b) / (size_a + size_b) * squared_distance(centroid_a, centroid_b)\n",
        "        raise ValueError('Enlace desconocido')\n",
        "\n",
        "    def _step(self):\n",
        "        active_ids = list(self.active.keys())\n",
        "        best_pair = None\n",
        "        best_dist = None\n",
        "        for idx_a in range(len(active_ids)):\n",
        "            for idx_b in range(idx_a + 1, len(active_ids)):\n",
        "                a = active_ids[idx_a]\n",
        "                b = active_ids[idx_b]\n",
        "                dist = self._cluster_distance(a, b)\n",
        "                if best_dist is None or dist < best_dist:\n",
        "                    best_dist = dist\n",
        "                    best_pair = (a, b)\n",
        "        a, b = best_pair\n",
        "        new_id = self.next_id\n",
        "        self.next_id += 1\n",
        "        merged_points = self.active[a] | self.active[b]\n",
        "        self.active[new_id] = merged_points\n",
        "        del self.active[a]\n",
        "        del self.active[b]\n",
        "        self.sizes[new_id] = len(merged_points)\n",
        "        total = [0.0] * len(self.matrix[0])\n",
        "        for idx in merged_points:\n",
        "            for j, value in enumerate(self.matrix[idx]):\n",
        "                total[j] += value\n",
        "        self.centroids[new_id] = [value / len(merged_points) for value in total]\n",
        "        self.merges.append((a, b, best_dist, len(merged_points)))\n",
        "\n",
        "    def fit(self, n_clusters):\n",
        "        while len(self.active) > n_clusters:\n",
        "            self._step()\n",
        "        clusters = list(self.active.values())\n",
        "        assignments = [None] * self.n\n",
        "        for cluster_idx, indices in enumerate(clusters):\n",
        "            for point in indices:\n",
        "                assignments[point] = cluster_idx\n",
        "        return assignments, self.merges\n",
        "\n",
        "def contingency_matrix(labels_true, labels_pred):\n",
        "    classes = sorted(set(labels_true))\n",
        "    clusters = sorted(set(labels_pred))\n",
        "    table = {cls: {cluster: 0 for cluster in clusters} for cls in classes}\n",
        "    for truth, pred in zip(labels_true, labels_pred):\n",
        "        table[truth][pred] += 1\n",
        "    return table\n",
        "\n",
        "def rand_scores(labels_true, labels_pred):\n",
        "    table = contingency_matrix(labels_true, labels_pred)\n",
        "    total_pairs = math.comb(len(labels_true), 2)\n",
        "    sum_comb_c = sum(math.comb(sum(row.values()), 2) for row in table.values())\n",
        "    cluster_sums = {}\n",
        "    for row in table.values():\n",
        "        for cluster, count in row.items():\n",
        "            cluster_sums[cluster] = cluster_sums.get(cluster, 0) + count\n",
        "    sum_comb_k = sum(math.comb(count, 2) for count in cluster_sums.values())\n",
        "    sum_comb = sum(math.comb(count, 2) for row in table.values() for count in row.values())\n",
        "    expected = (sum_comb_c * sum_comb_k) / total_pairs if total_pairs else 0.0\n",
        "    max_index = 0.5 * (sum_comb_c + sum_comb_k)\n",
        "    ari = (sum_comb - expected) / (max_index - expected) if max_index != expected else 0.0\n",
        "    ri = (sum_comb + (total_pairs - sum_comb_c - sum_comb_k + sum_comb)) / total_pairs if total_pairs else 0.0\n",
        "    return ri, ari\n",
        "\n",
        "def entropy(counts):\n",
        "    total = sum(counts)\n",
        "    if total == 0:\n",
        "        return 0.0\n",
        "    h = 0.0\n",
        "    for count in counts:\n",
        "        if count == 0:\n",
        "            continue\n",
        "        p = count / total\n",
        "        h -= p * math.log(p, 2)\n",
        "    return h\n",
        "\n",
        "def mutual_information(labels_true, labels_pred):\n",
        "    table = contingency_matrix(labels_true, labels_pred)\n",
        "    n = len(labels_true)\n",
        "    clusters = sorted(set(labels_pred))\n",
        "    class_totals = {cls: sum(row.values()) for cls, row in table.items()}\n",
        "    cluster_totals = {cluster: sum(table[cls][cluster] for cls in table) for cluster in clusters}\n",
        "    mi = 0.0\n",
        "    for cls, row in table.items():\n",
        "        for cluster, count in row.items():\n",
        "            if count == 0:\n",
        "                continue\n",
        "            mi += (count / n) * math.log((count * n) / (class_totals[cls] * cluster_totals[cluster]), 2)\n",
        "    return mi\n",
        "\n",
        "def homogeneity_completeness(labels_true, labels_pred):\n",
        "    table = contingency_matrix(labels_true, labels_pred)\n",
        "    n = len(labels_true)\n",
        "    classes = list(table.keys())\n",
        "    clusters = sorted(set(labels_pred))\n",
        "    class_totals = {cls: sum(table[cls].values()) for cls in classes}\n",
        "    cluster_totals = {cluster: sum(table[cls][cluster] for cls in classes) for cluster in clusters}\n",
        "    h_c = entropy(class_totals.values())\n",
        "    h_k = entropy(cluster_totals.values())\n",
        "    conditional_c = 0.0\n",
        "    for cluster in clusters:\n",
        "        for cls in classes:\n",
        "            count = table[cls][cluster]\n",
        "            if count == 0 or cluster_totals[cluster] == 0:\n",
        "                continue\n",
        "            conditional_c -= (count / n) * math.log(count / cluster_totals[cluster], 2)\n",
        "    conditional_k = 0.0\n",
        "    for cls in classes:\n",
        "        for cluster in clusters:\n",
        "            count = table[cls][cluster]\n",
        "            if count == 0 or class_totals[cls] == 0:\n",
        "                continue\n",
        "            conditional_k -= (count / n) * math.log(count / class_totals[cls], 2)\n",
        "    homogeneity = 1 - conditional_c / h_c if h_c > 0 else 1.0\n",
        "    completeness = 1 - conditional_k / h_k if h_k > 0 else 1.0\n",
        "    v_measure = (2 * homogeneity * completeness / (homogeneity + completeness)) if (homogeneity + completeness) > 0 else 0.0\n",
        "    return homogeneity, completeness, v_measure\n",
        "\n",
        "def silhouette_score(matrix, labels_pred):\n",
        "    n = len(matrix)\n",
        "    unique_clusters = sorted(set(labels_pred))\n",
        "    distances = pairwise_distances(matrix)\n",
        "    def dist(i, j):\n",
        "        if i == j:\n",
        "            return 0.0\n",
        "        return distances[(i, j)] if i < j else distances[(j, i)]\n",
        "    silhouettes = []\n",
        "    for i in range(n):\n",
        "        cluster = labels_pred[i]\n",
        "        same_cluster = [j for j in range(n) if labels_pred[j] == cluster and j != i]\n",
        "        a = sum(dist(i, j) for j in same_cluster) / len(same_cluster) if same_cluster else 0.0\n",
        "        b = None\n",
        "        for other in unique_clusters:\n",
        "            if other == cluster:\n",
        "                continue\n",
        "            members = [j for j in range(n) if labels_pred[j] == other]\n",
        "            if not members:\n",
        "                continue\n",
        "            avg = sum(dist(i, j) for j in members) / len(members)\n",
        "            if b is None or avg < b:\n",
        "                b = avg\n",
        "        if b is None:\n",
        "            silhouettes.append(0.0)\n",
        "        else:\n",
        "            denominator = max(a, b)\n",
        "            silhouettes.append(0.0 if denominator == 0 else (b - a) / denominator)\n",
        "    return sum(silhouettes) / len(silhouettes)\n",
        "\n",
        "def evaluate_linkage(matrix, labels_true, linkage, n_clusters=7):\n",
        "    model = AgglomerativeClusterer(matrix, linkage)\n",
        "    assignments, merges = model.fit(n_clusters)\n",
        "    ri, ari = rand_scores(labels_true, assignments)\n",
        "    mi = mutual_information(labels_true, assignments)\n",
        "    h, c, v = homogeneity_completeness(labels_true, assignments)\n",
        "    sil = silhouette_score(matrix, assignments)\n",
        "    return {\n",
        "        'ri': ri,\n",
        "        'ari': ari,\n",
        "        'mi': mi,\n",
        "        'homogeneity': h,\n",
        "        'completeness': c,\n",
        "        'v_measure': v,\n",
        "        'silhouette': sil,\n",
        "        'assignments': assignments,\n",
        "        'merges': merges\n",
        "    }\n",
        "\n",
        "def silhouette_by_cluster_count(matrix, max_clusters=10, linkage='ward'):\n",
        "    values = []\n",
        "    for clusters in range(2, max_clusters + 1):\n",
        "        model = AgglomerativeClusterer(matrix, linkage)\n",
        "        assignments, _ = model.fit(clusters)\n",
        "        score = silhouette_score(matrix, assignments)\n",
        "        values.append((clusters, score))\n",
        "    return values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "   Enlace |     RI |    ARI |     MI | Homog. | Compl. | V-meas. | Silhouette\n--------------------------------------------------------------------------------\n   single |  0.745 |  0.478 |  1.313 |  0.549 |  0.873 |  0.674 |      0.375\n complete |  0.968 |  0.911 |  1.987 |  0.831 |  0.855 |  0.843 |      0.565\n  average |  0.894 |  0.716 |  1.723 |  0.721 |  0.784 |  0.751 |      0.567\n     ward |  0.895 |  0.680 |  1.995 |  0.834 |  0.770 |  0.801 |      0.535\n\nSilueta promedio para enlace Ward variando el número de clústeres:\n-  2 clústeres -> silueta media 0.394\n-  3 clústeres -> silueta media 0.467\n-  4 clústeres -> silueta media 0.550\n-  5 clústeres -> silueta media 0.560\n-  6 clústeres -> silueta media 0.490\n-  7 clústeres -> silueta media 0.535\n-  8 clústeres -> silueta media 0.529\n-  9 clústeres -> silueta media 0.536\n- 10 clústeres -> silueta media 0.557\n\nPrimeras fusiones del dendrograma (enlace completo):\n- Fusionar 0 y 3 a distancia 0.000 genera un clúster de tamaño 2\n- Fusionar 1 y 5 a distancia 0.000 genera un clúster de tamaño 2\n- Fusionar 2 y 8 a distancia 0.000 genera un clúster de tamaño 2\n- Fusionar 4 y 10 a distancia 0.000 genera un clúster de tamaño 2\n- Fusionar 6 y 31 a distancia 0.000 genera un clúster de tamaño 2\n- Fusionar 11 y 20 a distancia 0.000 genera un clúster de tamaño 2\n- Fusionar 12 y 38 a distancia 0.000 genera un clúster de tamaño 2\n- Fusionar 15 y 46 a distancia 0.000 genera un clúster de tamaño 2\n- Fusionar 16 y 37 a distancia 0.000 genera un clúster de tamaño 2\n- Fusionar 17 y 22 a distancia 0.000 genera un clúster de tamaño 2\n"
        }
      ],
      "source": [
        "linkages = ['single', 'complete', 'average', 'ward']\n",
        "results = {}\n",
        "print(f\"{'Enlace':>9} | {'RI':>6} | {'ARI':>6} | {'MI':>6} | {'Homog.':>6} | {'Compl.':>6} | {'V-meas.':>6} | {'Silhouette':>10}\")\n",
        "print('-' * 80)\n",
        "for linkage in linkages:\n",
        "    metrics = evaluate_linkage(scaled_matrix, data_labels, linkage)\n",
        "    results[linkage] = metrics\n",
        "    print(\"{name:>9} | {ri:6.3f} | {ari:6.3f} | {mi:6.3f} | {h:6.3f} | {c:6.3f} | {v:6.3f} | {sil:10.3f}\".format(\n",
        "        name=linkage, ri=metrics['ri'], ari=metrics['ari'], mi=metrics['mi'],\n",
        "        h=metrics['homogeneity'], c=metrics['completeness'],\n",
        "        v=metrics['v_measure'], sil=metrics['silhouette']))\n",
        "\n",
        "print(\"\\nSilueta promedio para enlace Ward variando el número de clústeres:\")\n",
        "for clusters, score in silhouette_by_cluster_count(scaled_matrix, max_clusters=10, linkage='ward'):\n",
        "    print(f\"- {clusters:2d} clústeres -> silueta media {score:.3f}\")\n",
        "\n",
        "print(\"\\nPrimeras fusiones del dendrograma (enlace completo):\")\n",
        "for merge in results['complete']['merges'][:10]:\n",
        "    a, b, dist, size = merge\n",
        "    print(f\"- Fusionar {a} y {b} a distancia {dist:.3f} genera un clúster de tamaño {size}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Discusión.** El enlace completo ofrece el mejor balance entre métricas externas y cohesión interna (ARI ≈ 0.91). El enlace *ward* también logra buenos valores pero con una silueta ligeramente inferior. El análisis de silueta sugiere que entre 5 y 7 clústeres se alcanza el mejor compromiso.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Verificación del Problema 5 (DBSCAN)\n",
        "Se reproduce el ejercicio de DBSCAN del boletín teórico: 12 puntos en 2D con $\\varepsilon = 0.5$ y $\\text{MinPts} = 3$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Clústeres encontrados:\n- C0: P1, P2, P3\n- C1: P4, P5, P6\n- C2: P7, P8, P9\n- C3: P10, P11, P12\n\nPuntos núcleo: P1, P2, P3, P4, P5, P6, P7, P8, P9, P10, P11, P12\nPuntos de borde: Ninguno\nPuntos de ruido: Ninguno\n"
        }
      ],
      "source": [
        "points = {\n",
        "    'P1': (1.0, 1.2), 'P2': (0.8, 1.1), 'P3': (1.2, 0.9),\n",
        "    'P4': (8.0, 8.5), 'P5': (8.2, 8.3), 'P6': (7.9, 8.1),\n",
        "    'P7': (5.0, 1.0), 'P8': (5.2, 1.1), 'P9': (5.1, 0.9),\n",
        "    'P10': (3.0, 6.0), 'P11': (3.1, 6.2), 'P12': (2.9, 5.9)\n",
        "}\n",
        "labels_order = list(points.keys())\n",
        "coordinates = [points[label] for label in labels_order]\n",
        "\n",
        "def euclidean(a, b):\n",
        "    return math.sqrt((a[0] - b[0]) ** 2 + (a[1] - b[1]) ** 2)\n",
        "\n",
        "def region_query(index, eps):\n",
        "    neighbours = []\n",
        "    for j in range(len(coordinates)):\n",
        "        if euclidean(coordinates[index], coordinates[j]) <= eps:\n",
        "            neighbours.append(j)\n",
        "    return neighbours\n",
        "\n",
        "def dbscan(eps, min_pts):\n",
        "    n = len(coordinates)\n",
        "    visited = [False] * n\n",
        "    labels = [None] * n\n",
        "    neighbour_cache = [region_query(i, eps) for i in range(n)]\n",
        "    core_points = {i for i in range(n) if len(neighbour_cache[i]) >= min_pts}\n",
        "    cluster_id = 0\n",
        "    for i in range(n):\n",
        "        if visited[i]:\n",
        "            continue\n",
        "        visited[i] = True\n",
        "        if i not in core_points:\n",
        "            labels[i] = -1\n",
        "            continue\n",
        "        labels[i] = cluster_id\n",
        "        queue = [j for j in neighbour_cache[i] if j != i]\n",
        "        while queue:\n",
        "            j = queue.pop(0)\n",
        "            if not visited[j]:\n",
        "                visited[j] = True\n",
        "                if j in core_points:\n",
        "                    for q in neighbour_cache[j]:\n",
        "                        if q not in queue:\n",
        "                            queue.append(q)\n",
        "            if labels[j] in (None, -1):\n",
        "                labels[j] = cluster_id\n",
        "        cluster_id += 1\n",
        "    for i in range(n):\n",
        "        if labels[i] is None:\n",
        "            labels[i] = -1\n",
        "    border_points = {i for i in range(n) if labels[i] >= 0 and i not in core_points}\n",
        "    noise_points = {i for i, label in enumerate(labels) if label == -1}\n",
        "    return labels, core_points, border_points, noise_points\n",
        "\n",
        "labels_found, core_set, border_set, noise_set = dbscan(0.5, 3)\n",
        "clusters = {}\n",
        "for idx, cluster in enumerate(labels_found):\n",
        "    clusters.setdefault(cluster, []).append(labels_order[idx])\n",
        "print(\"Clústeres encontrados:\")\n",
        "for cluster_id, members in clusters.items():\n",
        "    cluster_name = f\"C{cluster_id}\" if cluster_id >= 0 else \"Ruido\"\n",
        "    print(f\"- {cluster_name}: {', '.join(members)}\")\n",
        "print(\"\\nPuntos núcleo:\", ', '.join(labels_order[i] for i in sorted(core_set)))\n",
        "print(\"Puntos de borde:\", ', '.join(labels_order[i] for i in sorted(border_set)) or 'Ninguno')\n",
        "print(\"Puntos de ruido:\", ', '.join(labels_order[i] for i in sorted(noise_set)) or 'Ninguno')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Resultado.** Se forman cuatro clústeres densos (`P1–P3`, `P4–P6`, `P7–P9`, `P10–P12`) y todos los puntos son núcleo, sin puntos de borde ni ruido.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Compresión de imágenes mediante K-Means\n",
        "Ante la ausencia de PIL, se trabaja con imágenes PPM sintéticas (gradiente, franjas y un paisaje).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Imágenes base generadas en 'prueba1/images'.\n"
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "def ensure_dir(path):\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "ensure_dir(Path('prueba1/images'))\n",
        "ensure_dir(Path('prueba1/reduced_images'))\n",
        "\n",
        "def gradient_image(width, height):\n",
        "    image = []\n",
        "    for y in range(height):\n",
        "        row = []\n",
        "        for x in range(width):\n",
        "            r = int(255 * x / (width - 1))\n",
        "            g = int(255 * y / (height - 1))\n",
        "            b = int(255 * (x + y) / (width + height - 2))\n",
        "            row.append((r, g, b))\n",
        "        image.append(row)\n",
        "    return image\n",
        "\n",
        "def stripes_image(width, height):\n",
        "    image = []\n",
        "    for y in range(height):\n",
        "        row = []\n",
        "        for x in range(width):\n",
        "            stripe = (x // max(1, width // 8)) % 3\n",
        "            if stripe == 0:\n",
        "                color = (220, 20, 60)\n",
        "            elif stripe == 1:\n",
        "                color = (30, 144, 255)\n",
        "            else:\n",
        "                color = (255, 215, 0)\n",
        "            row.append(color)\n",
        "        image.append(row)\n",
        "    return image\n",
        "\n",
        "def landscape_image(width, height):\n",
        "    image = []\n",
        "    horizon = height // 2\n",
        "    for y in range(height):\n",
        "        row = []\n",
        "        for x in range(width):\n",
        "            if y < horizon:\n",
        "                b = int(200 + 55 * y / max(1, horizon))\n",
        "                g = int(150 + 80 * y / max(1, horizon))\n",
        "                r = int(120 + 30 * y / max(1, horizon))\n",
        "            else:\n",
        "                factor = (y - horizon) / max(1, height - horizon - 1)\n",
        "                g = int(120 + 80 * (1 - factor))\n",
        "                r = int(40 + 40 * factor)\n",
        "                b = int(20 + 60 * (1 - factor))\n",
        "            row.append((r, g, b))\n",
        "        image.append(row)\n",
        "    return image\n",
        "\n",
        "def save_ppm(image, path):\n",
        "    height = len(image)\n",
        "    width = len(image[0])\n",
        "    with path.open('w') as f:\n",
        "        f.write(f\"P3\\n{width} {height}\\n255\\n\")\n",
        "        for row in image:\n",
        "            values = []\n",
        "            for r, g, b in row:\n",
        "                values.append(f\"{r} {g} {b}\")\n",
        "            f.write(' '.join(values) + '\\n')\n",
        "\n",
        "def load_image(path):\n",
        "    with Path(path).open() as f:\n",
        "        if f.readline().strip() != 'P3':\n",
        "            raise ValueError('Solo se admiten ficheros P3 PPM')\n",
        "        dims_line = f.readline().strip()\n",
        "        while dims_line.startswith('#'):\n",
        "            dims_line = f.readline().strip()\n",
        "        width, height = map(int, dims_line.split())\n",
        "        _ = int(f.readline().strip())\n",
        "        data = f.read().split()\n",
        "        pixels = []\n",
        "        idx = 0\n",
        "        for _ in range(height):\n",
        "            row = []\n",
        "            for _ in range(width):\n",
        "                r = int(data[idx]); g = int(data[idx + 1]); b = int(data[idx + 2])\n",
        "                idx += 3\n",
        "                row.append((r, g, b))\n",
        "            pixels.append(row)\n",
        "        return pixels\n",
        "\n",
        "def show_image(image):\n",
        "    gradient_chars = ' .:-=+*#%@'\n",
        "    lines = []\n",
        "    for row in image:\n",
        "        line = ''\n",
        "        for r, g, b in row:\n",
        "            brightness = (0.299 * r + 0.587 * g + 0.114 * b) / 255\n",
        "            index = min(len(gradient_chars) - 1, int(brightness * (len(gradient_chars) - 1)))\n",
        "            line += gradient_chars[index]\n",
        "        lines.append(line)\n",
        "    return '\\n'.join(lines)\n",
        "\n",
        "def save_image(image, path, k):\n",
        "    base = Path(path)\n",
        "    base.parent.mkdir(parents=True, exist_ok=True)\n",
        "    content = []\n",
        "    height = len(image)\n",
        "    width = len(image[0])\n",
        "    content.append(f\"P3\\n{width} {height}\\n255\")\n",
        "    for row in image:\n",
        "        content.append(' '.join(f\"{int(r)} {int(g)} {int(b)}\" for r, g, b in row))\n",
        "    text = '\\n'.join(content) + '\\n'\n",
        "    for suffix in ['.ppm', '.png.ppm', f'.k{k}.ppm']:\n",
        "        with base.with_suffix(suffix).open('w') as f:\n",
        "            f.write(text)\n",
        "\n",
        "def get_size(path):\n",
        "    return round(Path(path).stat().st_size / 1024, 3)\n",
        "\n",
        "def compress_image(image, k, seed=0):\n",
        "    height = len(image)\n",
        "    width = len(image[0])\n",
        "    flat = [list(pixel) for row in image for pixel in row]\n",
        "    assignments, centroids, sse = kmeans(flat, k, seed)\n",
        "    new_pixels = []\n",
        "    idx = 0\n",
        "    for _ in range(height):\n",
        "        row = []\n",
        "        for _ in range(width):\n",
        "            centroid = centroids[assignments[idx]]\n",
        "            row.append(tuple(int(round(value)) for value in centroid))\n",
        "            idx += 1\n",
        "        new_pixels.append(row)\n",
        "    return new_pixels, sse\n",
        "\n",
        "images = {\n",
        "    'gradient': gradient_image(32, 32),\n",
        "    'stripes': stripes_image(32, 32),\n",
        "    'landscape': landscape_image(48, 32)\n",
        "}\n",
        "for name, image in images.items():\n",
        "    save_ppm(image, Path('prueba1/images') / f'{name}.ppm')\n",
        "print(\"Imágenes base generadas en 'prueba1/images'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nResumen para la imagen 'gradient':\n   k |          SSE |  Tamaño (KB)\n------------------------------------\n   3 |   5748816.72 |       10.671\n   5 |   3090741.53 |       10.925\n  10 |   1477751.76 |       11.018\n  16 |    931578.78 |       10.757\n  20 |    736586.03 |       10.863\n  32 |    469494.70 |       10.837\n  50 |    304810.28 |       10.927\n  64 |    240669.42 |       10.872\n\nResumen para la imagen 'stripes':\n   k |          SSE |  Tamaño (KB)\n------------------------------------\n   3 |   6581760.00 |       10.388\n   5 |         0.00 |       10.388\n  10 |         0.00 |       10.388\n  16 |         0.00 |       10.388\n  20 |         0.00 |       10.388\n  32 |         0.00 |       10.388\n  50 |         0.00 |       10.388\n  64 |         0.00 |       10.388\n\nResumen para la imagen 'landscape':\n   k |          SSE |  Tamaño (KB)\n------------------------------------\n   5 |    290146.29 |       16.513\n  10 |    124312.34 |       16.513\n  20 |     36816.00 |       16.513\n\nPrevisualización ASCII de la imagen 'gradient' comprimida a k=5:\n::::::::::::::------------------\n::::::::::::::------------------\n::::::::::::::------------------\n::::::::::::::------------------\n::::::::::::::------------------\n::::::::::::::------------------\n::::::::::::::-----------------+\n::::::::::::::---------------+++\n::::::::::::::-------------+++++\n::::::::::::::-----------+++++++\n::::::::::::::---------+++++++++\n::::::::::::::-------+++++++++++\n"
        }
      ],
      "source": [
        "portrait_k = [3, 5, 10, 16, 20, 32, 50, 64]\n",
        "landscape_k = [5, 10, 20]\n",
        "results = {}\n",
        "for name, original in images.items():\n",
        "    ks = landscape_k if name == 'landscape' else portrait_k\n",
        "    image_results = []\n",
        "    for k in ks:\n",
        "        compressed, sse = compress_image(original, k, seed=42)\n",
        "        base_path = Path('prueba1/reduced_images') / f'{name}_k{k}'\n",
        "        save_image(compressed, base_path, k)\n",
        "        size_kb = get_size(base_path.with_suffix(f'.k{k}.ppm'))\n",
        "        image_results.append({'k': k, 'sse': sse, 'size_kb': size_kb})\n",
        "    results[name] = image_results\n",
        "\n",
        "for name, entries in results.items():\n",
        "    print(f\"\\nResumen para la imagen '{name}':\")\n",
        "    print(f\"{'k':>4} | {'SSE':>12} | {'Tamaño (KB)':>12}\")\n",
        "    print('-' * 36)\n",
        "    for entry in entries:\n",
        "        print(f\"{entry['k']:>4} | {entry['sse']:12.2f} | {entry['size_kb']:12.3f}\")\n",
        "\n",
        "print(\"\\nPrevisualización ASCII de la imagen 'gradient' comprimida a k=5:\")\n",
        "preview_image, _ = compress_image(images['gradient'], 5, seed=42)\n",
        "for line in show_image(preview_image).split('\\n')[:12]:\n",
        "    print(line[:40])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretación.** Las franjas tienen muy pocos colores distintos, por lo que el tamaño del fichero casi no varía al cambiar $k$. El gradiente y el paisaje sí presentan variaciones graduales: al reducir el número de prototipos la SSE aumenta y el tamaño en disco oscila ligeramente, aunque al trabajar con formato PPM los cambios son más suaves que en PNG/JPG.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Reducción de dimensionalidad con PCA\n",
        "Se generan \"rostros\" sintéticos en una rejilla 8×8 y se aplica PCA paso a paso.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_base_patterns(size=8):\n",
        "    patterns = []\n",
        "    pattern1 = [(y + 1) / size for y in range(size) for x in range(size)]\n",
        "    pattern2 = [(x + 1) / size for y in range(size) for x in range(size)]\n",
        "    pattern3 = [(x + y) / (2 * size) for y in range(size) for x in range(size)]\n",
        "    patterns.extend([pattern1, pattern2, pattern3])\n",
        "    return patterns\n",
        "\n",
        "base_patterns = generate_base_patterns()\n",
        "\n",
        "def add_noise(pattern, level=0.1, rng=None):\n",
        "    if rng is None:\n",
        "        rng = random.Random()\n",
        "    noisy = []\n",
        "    for value in pattern:\n",
        "        noise = (rng.random() - 0.5) * 2 * level\n",
        "        noisy.append(min(max(value + noise, 0.0), 1.0))\n",
        "    return noisy\n",
        "\n",
        "def generate_face_dataset(samples=90, size=8, seed=123):\n",
        "    rng = random.Random(seed)\n",
        "    data, labels = [], []\n",
        "    for idx in range(samples):\n",
        "        base_idx = idx % len(base_patterns)\n",
        "        data.append(add_noise(base_patterns[base_idx], level=0.1, rng=rng))\n",
        "        labels.append(base_idx)\n",
        "    return data, labels\n",
        "\n",
        "def mean_vector(data):\n",
        "    length = len(data[0])\n",
        "    mean = [0.0] * length\n",
        "    for vector in data:\n",
        "        for i, value in enumerate(vector):\n",
        "            mean[i] += value\n",
        "    return [value / len(data) for value in mean]\n",
        "\n",
        "def center_data(data, mean):\n",
        "    return [[value - mean[i] for i, value in enumerate(vector)] for vector in data]\n",
        "\n",
        "def covariance_matrix(data):\n",
        "    n = len(data)\n",
        "    dim = len(data[0])\n",
        "    cov = [[0.0] * dim for _ in range(dim)]\n",
        "    for vector in data:\n",
        "        for i in range(dim):\n",
        "            for j in range(i, dim):\n",
        "                cov[i][j] += vector[i] * vector[j]\n",
        "    for i in range(dim):\n",
        "        for j in range(i, dim):\n",
        "            cov_val = cov[i][j] / (n - 1 if n > 1 else 1)\n",
        "            cov[i][j] = cov_val\n",
        "            cov[j][i] = cov_val\n",
        "    return cov\n",
        "\n",
        "def mat_vec_mul(matrix, vector):\n",
        "    return [sum(row[j] * vector[j] for j in range(len(vector))) for row in matrix]\n",
        "\n",
        "def dot(a, b):\n",
        "    return sum(x * y for x, y in zip(a, b))\n",
        "\n",
        "def normalize(vector):\n",
        "    norm = math.sqrt(sum(x * x for x in vector))\n",
        "    if norm == 0:\n",
        "        return vector[:]\n",
        "    return [x / norm for x in vector]\n",
        "\n",
        "def power_iteration(matrix, iterations=1000, tolerance=1e-9, seed=0):\n",
        "    rng = random.Random(seed)\n",
        "    n = len(matrix)\n",
        "    vec = [rng.random() for _ in range(n)]\n",
        "    vec = normalize(vec)\n",
        "    for _ in range(iterations):\n",
        "        next_vec = normalize(mat_vec_mul(matrix, vec))\n",
        "        diff = max(abs(next_vec[i] - vec[i]) for i in range(n))\n",
        "        vec = next_vec\n",
        "        if diff < tolerance:\n",
        "            break\n",
        "    eigenvalue = dot(vec, mat_vec_mul(matrix, vec))\n",
        "    return eigenvalue, vec\n",
        "\n",
        "def deflate(matrix, eigenvalue, eigenvector):\n",
        "    n = len(matrix)\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            matrix[i][j] -= eigenvalue * eigenvector[i] * eigenvector[j]\n",
        "\n",
        "def pca(data, components_count):\n",
        "    mean = mean_vector(data)\n",
        "    centered = center_data(data, mean)\n",
        "    cov = covariance_matrix(centered)\n",
        "    working = [row[:] for row in cov]\n",
        "    components = []\n",
        "    eigenvalues = []\n",
        "    for idx in range(components_count):\n",
        "        eigenvalue, eigenvector = power_iteration(working, seed=idx)\n",
        "        components.append(eigenvector)\n",
        "        eigenvalues.append(eigenvalue)\n",
        "        deflate(working, eigenvalue, eigenvector)\n",
        "    return mean, components, eigenvalues\n",
        "\n",
        "def project(data, mean, components):\n",
        "    centered = center_data(data, mean)\n",
        "    return [[dot(vector, component) for component in components] for vector in centered]\n",
        "\n",
        "def reconstruct(projections, mean, components):\n",
        "    reconstructions = []\n",
        "    dim = len(mean)\n",
        "    for coords in projections:\n",
        "        vector = mean[:]\n",
        "        for weight, component in zip(coords, components):\n",
        "            for i in range(dim):\n",
        "                vector[i] += weight * component[i]\n",
        "        reconstructions.append(vector)\n",
        "    return reconstructions\n",
        "\n",
        "def vector_to_ascii(vector, size=8):\n",
        "    gradient_chars = ' .:-=+*#%@'\n",
        "    lines = []\n",
        "    for row_idx in range(size):\n",
        "        line = ''\n",
        "        for col_idx in range(size):\n",
        "            value = vector[row_idx * size + col_idx]\n",
        "            index = min(len(gradient_chars) - 1, max(0, int(value * (len(gradient_chars) - 1))))\n",
        "            line += gradient_chars[index]\n",
        "        lines.append(line)\n",
        "    return '\\n'.join(lines)\n",
        "\n",
        "def display_faces(vectors, labels, count, title):\n",
        "    print(title)\n",
        "    print('-' * len(title))\n",
        "    for idx in range(min(count, len(vectors))):\n",
        "        print(f\"Rostro {idx + 1} (clase {labels[idx]}):\")\n",
        "        print(vector_to_ascii(vectors[idx]))\n",
        "        print()\n",
        "\n",
        "def train_test_split(data, labels, test_ratio=0.3, seed=321):\n",
        "    indices = list(range(len(data)))\n",
        "    rng = random.Random(seed)\n",
        "    rng.shuffle(indices)\n",
        "    split = int(len(data) * (1 - test_ratio))\n",
        "    train_idx = indices[:split]\n",
        "    test_idx = indices[split:]\n",
        "    train_data = [data[i] for i in train_idx]\n",
        "    train_labels = [labels[i] for i in train_idx]\n",
        "    test_data = [data[i] for i in test_idx]\n",
        "    test_labels = [labels[i] for i in test_idx]\n",
        "    return train_data, train_labels, test_data, test_labels\n",
        "\n",
        "def euclidean_distance(a, b):\n",
        "    return math.sqrt(sum((x - y) ** 2 for x, y in zip(a, b)))\n",
        "\n",
        "def knn_predict(train_data, train_labels, sample, k=3):\n",
        "    distances = [(euclidean_distance(vector, sample), label) for vector, label in zip(train_data, train_labels)]\n",
        "    distances.sort(key=lambda item: item[0])\n",
        "    votes = {}\n",
        "    for _, label in distances[:k]:\n",
        "        votes[label] = votes.get(label, 0) + 1\n",
        "    return max(votes.items(), key=lambda item: (item[1], -item[0]))[0]\n",
        "\n",
        "def accuracy(train_data, train_labels, test_data, test_labels, k=3):\n",
        "    correct = 0\n",
        "    for sample, label in zip(test_data, test_labels):\n",
        "        if knn_predict(train_data, train_labels, sample, k) == label:\n",
        "            correct += 1\n",
        "    return correct / len(test_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Autovalores y varianza explicada:\nComp. |    Autovalor |   Var. % | Var. acum. %\n---------------------------------------------\n    1 |       1.7223 |    85.99 |        85.99\n    2 |       0.2138 |    10.67 |        96.67\n    3 |       0.0101 |     0.50 |        97.17\n    4 |       0.0094 |     0.47 |        97.64\n    5 |       0.0087 |     0.44 |        98.08\n    6 |       0.0084 |     0.42 |        98.49\n    7 |       0.0081 |     0.41 |        98.90\n    8 |       0.0076 |     0.38 |        99.28\n    9 |       0.0073 |     0.37 |        99.64\n   10 |       0.0071 |     0.36 |       100.00\n"
        }
      ],
      "source": [
        "faces, face_labels = generate_face_dataset()\n",
        "mean_face, components, eigenvalues = pca(faces, components_count=10)\n",
        "projections = project(faces, mean_face, components)\n",
        "reconstructed = reconstruct(projections, mean_face, components)\n",
        "\n",
        "total_variance = sum(eigenvalues)\n",
        "print(\"Autovalores y varianza explicada:\")\n",
        "print(f\"{'Comp.':>5} | {'Autovalor':>12} | {'Var. %':>8} | {'Var. acum. %':>12}\")\n",
        "print('-' * 45)\n",
        "cumulative = 0.0\n",
        "for idx, eigenvalue in enumerate(eigenvalues, start=1):\n",
        "    ratio = eigenvalue / total_variance if total_variance else 0.0\n",
        "    cumulative += ratio\n",
        "    print(f\"{idx:>5} | {eigenvalue:12.4f} | {ratio * 100:8.2f} | {cumulative * 100:12.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Primeros 25 rostros originales (escala ASCII)\n---------------------------------------------\nRostro 1 (clase 0):\n    . . \n:.....:.\n-:--=::-\n-+==+==+\n+++*++*+\n#*##*#*+\n%#%%%###\n%%@%%%@%\n\nRostro 2 (clase 1):\n .-=**%@\n..--**%%\n .:=+*#%\n .:==*#@\n -:=**#@\n.:-=+*#%\n..-==#%@\n .==+##%\n\nRostro 3 (clase 2):\n   ::---\n. .:::-=\n :.:--==\n:.::=+=+\n.:-=+++*\n:-==++**\n:--+***#\n=-=*+###\n\nRostro 4 (clase 0):\n. .. .  \n::.::-..\n--=:-:-=\n========\n*+*+=+*+\n+#*##*#*\n%%%#%%##\n@%%%%%@%\n\nRostro 5 (clase 1):\n .-=*##@\n..:=+*#@\n --+=*%%\n ::-++%%\n..-==#%@\n..:-+##%\n .:-+*#%\n.::-*##%\n\nRostro 6 (clase 2):\n   ..:=-\n....:-=-\n .:.---=\n.--:-==+\n.--===++\n.--++=+#\n:-+++*##\n-=++*##%\n\nRostro 7 (clase 0):\n .  .. .\n::.::.:.\n:--:==::\n==+=+=-+\n++*==+=+\n*****#*#\n%%%#####\n@@@@%@@%\n\nRostro 8 (clase 1):\n .=+=##%\n :--=##@\n.::=+##%\n .==+*%@\n :-+=*#@\n.:-=*##%\n..:=+##@\n.:==+*%@\n\nRostro 9 (clase 2):\n   .::-=\n  .::-==\n:::::=++\n.:---=++\n::-==+++\n:--==++#\n:=++***#\n-=++**#%\n\nRostro 10 (clase 0):\n.. . ...\n:.-.:.::\n:-:=:=-:\n-=====+=\n++*+*+==\n#*##*#*#\n%%%###%%\n%@%@%@%%\n\nRostro 11 (clase 1):\n .:=*+#%\n..:=+*#%\n -:=+*#@\n :=++*#%\n.--+**#@\n :--+*%%\n..-++##%\n..-=+*#@\n\nRostro 12 (clase 2):\n ..:.:--\n...:::-=\n..:---=+\n.::--=+=\n.-:=+=++\n:--==**#\n---=+++*\n--+++**%\n\nRostro 13 (clase 0):\n.  . ...\n-:.:::..\n---=-=:-\n=+-=--==\n+++++**=\n***#*#+#\n##%#####\n@@%%%@%%\n\nRostro 14 (clase 1):\n.:-++*%%\n..:=+*#%\n ::==##@\n ::=*##@\n..=++*#@\n.:-=+*%%\n .:=+##@\n .-=+*%@\n\nRostro 15 (clase 2):\n   :-::-\n. ::::-=\n  :--=++\n...=-=++\n.---==+*\n.-===*+*\n---++#*#\n-==**##%\n\nRostro 16 (clase 0):\n..  ..  \n::..-::.\n---:-::-\n====-=-=\n*=+=++**\n+#*####*\n%####%##\n@@@@%%%@\n\nRostro 17 (clase 1):\n..-=*#%%\n -:-**#%\n .:-=*#%\n --=+*#%\n..=+=*#@\n .:++*#%\n.:-++##%\n..-=+*#@\n\nRostro 18 (clase 2):\n . ..-:-\n. .:-:==\n.:.-===+\n:::=-+=+\n.----==#\n--==+***\n====+***\n===++*%%\n\nRostro 19 (clase 0):\n .... . \n:.::-:.:\n:--=----\n=+=-====\n+*+++=++\n*#**+**#\n%%*#*%##\n%%@%@@@%\n\nRostro 20 (clase 1):\n.:--*#%@\n..--**#@\n.:-==*#%\n.--=+*#%\n.-:++*%@\n..=++*#@\n.:-==*#@\n.:==**#@\n\nRostro 21 (clase 2):\n ..::-=-\n  .::---\n..::--=+\n:.:--==+\n:--==+*#\n:==++++#\n:=-=+*#*\n===+**%#\n\nRostro 22 (clase 0):\n     ...\n:.--:::.\n:----=:-\n+=-==-==\n=+*+++++\n*#*##*#*\n##%%#%%#\n@@%%@@%%\n\nRostro 23 (clase 1):\n.:-=**#%\n.::-+*#@\n .-++*%@\n.:=-**%@\n ::+*##%\n.:-=++#%\n.:--+*#@\n .==**#%\n\nRostro 24 (clase 2):\n   :.:=-\n  .::--=\n..::--==\n.:---=++\n:::--=*+\n::==+++*\n--===+*#\n==+***%#\n\nRostro 25 (clase 0):\n.     ..\n.::.-.-:\n-=-::-:-\n=++--=+-\n*=++=*++\n+#**#***\n######%#\n%%%%@%@%\n\n"
        }
      ],
      "source": [
        "display_faces(faces, face_labels, count=25, title='Primeros 25 rostros originales (escala ASCII)')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Primeros 25 rostros reconstruidos con 10 componentes\n----------------------------------------------------\nRostro 1 (clase 0):\n.  .. ..\n:::::.:.\n----:::-\n======-=\n++++++++\n***#****\n%#%%#%##\n%%@%@%%%\n\nRostro 2 (clase 1):\n :-=**%%\n..-=**%%\n .-=+*#%\n :-=+*#%\n :-=+*%%\n.:-=+##%\n..-++*%%\n :-=+*#%\n\nRostro 3 (clase 2):\n  .::--=\n. .::-==\n.:.:--=+\n:.:-===+\n::-==++*\n:-==++**\n--=++**#\n-==+**##\n\nRostro 4 (clase 0):\n.... .. \n::.:::..\n--------\n=+===-==\n*+++++*+\n****##**\n#%%#%%##\n@%%%%%%@\n\nRostro 5 (clase 1):\n .-=*##%\n .:=+*#@\n :-++##%\n ::-+*#%\n..-==#%@\n.::=+##%\n .--+*#%\n.::=+*#%\n\nRostro 6 (clase 2):\n   ...=-\n .:::-==\n .:.---=\n.:-:-==+\n:--==+++\n:--++++*\n--=*+*##\n-=++**##\n\nRostro 7 (clase 0):\n.. . . .\n:::::::.\n------::\n======-=\n+++++=++\n********\n%%#%##%#\n@%%%%%@%\n\nRostro 8 (clase 1):\n .-=+##%\n :--+##@\n :-=+###\n.:-=+*%%\n :-=+##%\n.:-=+*#%\n.::=+*%%\n.:-=+*#%\n\nRostro 9 (clase 2):\n   ..:--\n ..::-==\n.::-:-=+\n.::-==++\n::-==++*\n:-==++*#\n--+=+**#\n-==+**#%\n\nRostro 10 (clase 0):\n..  ....\n::-:::::\n--------\n======+=\n++++++++\n#*#*****\n%#####%%\n%@@%%%%%\n\nRostro 11 (clase 1):\n .:=+*#%\n.::=+*#%\n :-=+*#%\n :-=*##%\n.:-=+*#%\n :-=+*#%\n.:-++##%\n .-=+*#%\n\nRostro 12 (clase 2):\n . :.:--\n ..::--=\n..:---=+\n.::--=++\n:-:===++\n:--==++*\n--==+**#\n-=++***#\n\nRostro 13 (clase 0):\n.... . .\n::::::.:\n--------\n=+-==-==\n++++++*+\n***###**\n#%%#%%##\n@%%%%@%%\n\nRostro 14 (clase 1):\n .-=+*%%\n..-=+*%%\n :-=+*#%\n .-=**#%\n..-=+*#@\n :--+*%%\n :-=+##%\n .:=+*%%\n\nRostro 15 (clase 2):\n   ::::=\n. .:-:-=\n..:---=+\n.:.-===+\n.--===+*\n:--==++*\n--=++**#\n-=++***#\n\nRostro 16 (clase 0):\n. . ....\n::.::::.\n---:--:-\n=+====-=\n++++++*+\n***###**\n%%###%%#\n@%%%%%%%\n\nRostro 17 (clase 1):\n .-=**%%\n.::=+*#%\n..-=++#%\n :-=+*#%\n.::++*%%\n :-=+##%\n.:-=+*#%\n.:-=+*#%\n\nRostro 18 (clase 2):\n  .:.-:-\n  .:---=\n..:--==+\n:::-===+\n::--==+*\n--==++**\n-===+**#\n===+**##\n\nRostro 19 (clase 0):\n.. .....\n:::.::::\n-:------\n========\n++++++++\n********\n%#######\n%%@%%%%%\n\nRostro 20 (clase 1):\n.:==*#%%\n.:-=*+#%\n..-==*#%\n.:-=+*%%\n.:-++*%%\n.:-=+*#%\n.:-=+*%%\n.:==+*#%\n\nRostro 21 (clase 2):\n  ..:---\n  .:--==\n..::--=+\n.::--==+\n::-==+**\n:--=++*#\n-===+*##\n===+**#%\n\nRostro 22 (clase 0):\n.    . .\n:.::::::\n:-----:-\n========\n++*=++++\n#***#*#*\n##%%#%%%\n@%%%%@@%\n\nRostro 23 (clase 1):\n :-=+*#%\n :-=+*#@\n.:-=+*%%\n :-=+#%%\n :-++*#%\n :-==*#%\n.:-=+*#%\n :-=+*#%\n\nRostro 24 (clase 2):\n  ...:--\n  ..-:-=\n...:---=\n::---==+\n:-:===++\n:-==+++*\n---=++##\n-=++**##\n\nRostro 25 (clase 0):\n...  .. \n.::.::::\n----:---\n==+-====\n++++++++\n****#**#\n######%#\n%%%%%%%@\n\n"
        }
      ],
      "source": [
        "display_faces(reconstructed, face_labels, count=25, title='Primeros 25 rostros reconstruidos con 10 componentes')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Exactitud k-NN con todas las características: 1.000\nExactitud k-NN tras PCA (3 componentes): 1.000\n"
        }
      ],
      "source": [
        "train_data, train_labels, test_data, test_labels = train_test_split(faces, face_labels)\n",
        "baseline_acc = accuracy(train_data, train_labels, test_data, test_labels)\n",
        "proj_train = project(train_data, mean_face, components[:3])\n",
        "proj_test = project(test_data, mean_face, components[:3])\n",
        "pca_acc = accuracy(proj_train, train_labels, proj_test, test_labels)\n",
        "print(f\"Exactitud k-NN con todas las características: {baseline_acc:.3f}\")\n",
        "print(f\"Exactitud k-NN tras PCA (3 componentes): {pca_acc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Conclusiones.** Las tres primeras componentes explican más del 90% de la varianza del conjunto sintético. Las reconstrucciones mantienen la estructura general de los patrones y el clasificador $k$-NN conserva la exactitud tras proyectar en 3 componentes, lo que avala el uso de PCA como preprocesado.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}